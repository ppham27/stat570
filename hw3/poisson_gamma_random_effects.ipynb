{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'invalid': 'raise', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.seterr(divide='raise', invalid='raise', over='warn', under='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_0 = -2\n",
    "BETA_1 = np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(b, n):\n",
    "    x = stats.norm.rvs(loc=0, scale=1, size=n)    \n",
    "    mu = np.exp(x*BETA_1 + BETA_0)\n",
    "    theta = stats.gamma.rvs(b, scale=1/b, size=n)\n",
    "    return x, stats.poisson.rvs(theta*mu, size=n)\n",
    "\n",
    "def estimate_by_poisson_model(x, y):\n",
    "    def score(beta):\n",
    "        beta_0 = beta[0]\n",
    "        beta_1 = beta[1]\n",
    "        return np.array([\n",
    "            np.sum(y - np.exp(beta_0 + beta_1*x)),\n",
    "            np.sum(x*y - x*np.exp(beta_0 + beta_1*x)),\n",
    "        ])\n",
    "    \n",
    "    beta_hat = optimize.root(score, np.array([-2, np.log(2)]))['x']        \n",
    "    mu_hat = np.exp(beta_hat[0] + beta_hat[1]*x)\n",
    "    \n",
    "    fisher_determinant = np.sum(mu_hat)*np.sum(x*x*mu_hat) - np.square(np.sum(x*mu_hat))\n",
    "    beta_hat_variance = np.array([\n",
    "        [np.sum(x*x*mu_hat), -np.sum(x*mu_hat)],\n",
    "        [-np.sum(x*mu_hat), np.sum(mu_hat)]\n",
    "    ])/fisher_determinant\n",
    "    \n",
    "    return beta_hat, beta_hat_variance\n",
    "\n",
    "def estimate_by_quasi_likelihood(x, y):\n",
    "    beta_hat, beta_hat_variance = estimate_by_poisson_model(x, y)\n",
    "    \n",
    "    mu_hat = np.exp(beta_hat[0] + beta_hat[1]*x)\n",
    "    alpha_hat = np.sum(np.square(y - mu_hat)/mu_hat)/(len(y) - 2)\n",
    "    \n",
    "    return beta_hat, alpha_hat*beta_hat_variance    \n",
    "\n",
    "def estimate_by_sandwich(x, y):\n",
    "    beta_hat, beta_hat_variance = estimate_by_poisson_model(x, y)\n",
    "    \n",
    "    mu_hat = np.exp(beta_hat[0] + beta_hat[1]*x)        \n",
    "    b_hat = np.sum(np.square(y - mu_hat)[:,np.newaxis,np.newaxis]*np.transpose(np.array([\n",
    "        [np.ones_like(y), x],\n",
    "        [x, x*x],\n",
    "    ]), axes=[2,1,0]), axis=0)\n",
    "    \n",
    "    return beta_hat, beta_hat_variance.dot(b_hat).dot(beta_hat_variance)\n",
    "\n",
    "def is_covered_by_confidence_interval(estimates, actual, variances, level=0.95):\n",
    "    return np.abs(estimates - actual) <= stats.norm.isf((1 - level)/2)*np.sqrt(np.diag(variances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = pd.DataFrame(index=pd.MultiIndex.from_product([\n",
    "    [0.2, 1, 10, 1000],\n",
    "    [10, 20, 50, 100, 250],\n",
    "], names=['b', 'n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=0.2, n=10\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from scipy import optimize\n",
    "\n",
    "np.random.seed(2018)\n",
    "for b, n in experiments.index:\n",
    "    print('b={}, n={}'.format(b, n))\n",
    "    poisson_estimates = []\n",
    "    poisson_variances = []\n",
    "    \n",
    "    quasi_likelihood_estimates = []\n",
    "    quasi_likelihood_variances = []\n",
    "    \n",
    "    sandwich_estimates = []\n",
    "    sandwich_variances = []\n",
    "    \n",
    "    for i in range(10000):\n",
    "        x, y = generate_data(b, n)\n",
    "        while np.sum(y > 0) < 2:\n",
    "            #logging.getLogger().warning('Regenerating data.')\n",
    "            x, y = generate_data(b, n)\n",
    "\n",
    "        poisson_estimate, poisson_variance = estimate_by_poisson_model(x, y)            \n",
    "        quasi_likelihood_estimate, quasi_likelihood_variance = estimate_by_quasi_likelihood(x, y)\n",
    "        sandwich_estimate, sandwich_variance = estimate_by_sandwich(x, y)\n",
    "                    \n",
    "        poisson_estimates.append(poisson_estimate)\n",
    "        poisson_variances.append(poisson_variance)        \n",
    "        \n",
    "        quasi_likelihood_estimates.append(quasi_likelihood_estimate)\n",
    "        quasi_likelihood_variances.append(quasi_likelihood_variance)\n",
    "        \n",
    "        sandwich_estimates.append(sandwich_estimate)\n",
    "        sandwich_variances.append(sandwich_variance)\n",
    "        \n",
    "    poisson_estimates = np.array(poisson_estimates)\n",
    "    poisson_variances = np.array(poisson_variances)\n",
    "    \n",
    "    quasi_likelihood_estimates = np.array(quasi_likelihood_estimates)\n",
    "    quasi_likelihood_variances = np.array(quasi_likelihood_variances)\n",
    "    \n",
    "    sandwich_estimates = np.array(sandwich_estimates)\n",
    "    sandwich_variances = np.array(sandwich_variances)\n",
    "    \n",
    "    is_covered_by_confidence_interval_vectorized = (\n",
    "        np.vectorize(\n",
    "            lambda e, v: is_covered_by_confidence_interval(\n",
    "                e, np.array([BETA_0, BETA_1]), v),\n",
    "            otypes=[np.bool],\n",
    "            signature='(i),(i,i)->(i)'))\n",
    "    \n",
    "    poisson_coverage = np.sum(is_covered_by_confidence_interval_vectorized(\n",
    "        poisson_estimates, poisson_variances), axis=0)/len(poisson_estimates)\n",
    "    \n",
    "    quasi_likelihood_coverage = np.sum(is_covered_by_confidence_interval_vectorized(\n",
    "        quasi_likelihood_estimates, quasi_likelihood_variances), axis=0)/len(quasi_likelihood_estimates)\n",
    "    \n",
    "    sandwich_coverage = np.sum(is_covered_by_confidence_interval_vectorized(\n",
    "        sandwich_estimates, sandwich_variances), axis=0)/len(sandwich_estimates)\n",
    "    \n",
    "    print(poisson_coverage)\n",
    "    print(quasi_likelihood_coverage)\n",
    "    print(sandwich_coverage)\n",
    "    \n",
    "    #print(np.sum(np.abs(estimates[:,0] - BETA_0) <= np.sqrt(variances[:,0])*stats.norm.isf((1 - 0.95)/2)))\n",
    "    #print(np.sum(np.abs(estimates[:,1] - BETA_1) <= np.sqrt(variances[:,1])*stats.norm.isf((1 - 0.95)/2)))    \n",
    "    #print(np.mean(estimates, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
