\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{subcaption}

\title{Coursework 3: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle

\begin{enumerate}
\item Consider the Poisson-gamma random effects model given by
  \begin{align}
    Y_i \mid \mu_i, \theta_i
    &\sim \operatorname{Poisson}\left(\mu_i\theta_i\right) \\
    \theta_i
    &\sim \operatorname{Gamma}\left(b, b\right),
  \end{align}
  which leads to a negative binomial marginal model with the variance a
  quadratic function of the mean. Design a simulation study, along the lines of
  that which produced Table 2.3 in the book (overdispersed Poisson example) to
  investigate the efficiency and robustness under
  \begin{itemize}
  \item a Poisson model;
  \item quasi-likelihood with $\mathbb{E}\left[Y\right] = \mu$ and
    $\operatorname{Var}\left(Y\right) = \alpha\mu$; and
  \item sandwich estimation.
  \end{itemize}

  Use a log-linear model
  \begin{equation}
    \log \mu_i = \beta_0 + \beta_1x_i,
  \end{equation}
  with $x_i \sim_\mathrm{iid} \mathcal{N}\left(0, 1\right)$ for
  $i = 1,2,\ldots,n$, and $\beta_0 = -2$ and $\beta_1 = \log 2$.

  Simulate for:
  \begin{itemize}    
  \item $b \in \left\{0.2,1,10,1000\right\}$.
  \item $n \in \left\{10, 20, 50, 100, 250\right\}$.
  \end{itemize}

  Summarize what your take away message is after carrying out these simulations.

  \begin{description}
  \item[Solution:] Note that
    \begin{align}
      \mathbb{P}\left(
      Y_i = y \mid \mu_i
      \right)
      &= \int_0^\infty\mathbb{P}\left(
      Y_i = y \mid \mu_i, \theta_i = \theta
      \right)\mathbb{P}\left(
      \theta_i = \theta
      \mid b
        \right)
        \,\mathrm{d}\theta \nonumber\\
      &= \int_0^\infty
        \left(
        \frac{\left(\mu_i\theta\right)^y}{y!}\exp\left(-\mu_i\theta\right)
        \right)
        \left(
        \frac{b^b}{\Gamma(b)}\theta^{b-1}
        \exp\left(-b\theta\right)
        \right)        
        \,\mathrm{d}\theta \nonumber\\
      &= \frac{\mu_i^y b^b}{y!\Gamma(b)}
        \int_0^\infty
        \theta^{b + y - 1}\exp\left(-\theta(b + \mu_i)\right)
        \,\mathrm{d}\theta \nonumber\\
      &= \frac{\Gamma(y + b)}{y!\Gamma(b)}
        \frac{\mu_i^y b^b}{\left(\mu_i + b\right)^{b + y}}
        = \frac{\Gamma(y + b)}{y!\Gamma(b)}
        \left(\frac{b}{\mu_i + b}\right)^b
        \left(\frac{\mu_i}{\mu_i + b}\right)^y
        \nonumber\\
      &\sim \operatorname{NegativeBinomial}\left(
        b,
        \frac{\mu_i}{\mu_i + b}
        \right).
        \label{eqn:p1_negative_binomial}
    \end{align}

    By properties of the negative binomial distribution, we have that
    \begin{align}
      \mathbb{E}\left[
      Y_i
      \mid x_i
      \right]
      &= \mu_i = \exp\left(\beta_0 + \beta_1x_i\right) \nonumber\\
      \operatorname{Var}\left(Y_i \mid x_i\right)
      &= \mu_i\left(
        1 + \frac{\mu_i}{b}
        \right).
      \label{eqn:p1_y_mean_variance}
    \end{align}
    Thus, smaller values of $b$ correspond to more dispersion.

    \subsection*{Poisson Model}

    In the Poisson model, we assume that
    $\operatorname{Var}\left(Y_i \mid x_i\right) = \mu_i$, e.g.
    $b \rightarrow \infty$, so we neglect the overdispersion parameter.

    In this case, the log-likelihood function is
    \begin{equation}
      l\left(\beta\right) =
      \sum_{i=1}^n\left[
        y_i\left(\beta_0 + \beta_1x_i\right) -
        \exp\left(\beta_0 + \beta_1x_i\right) -
        \sum_{k=1}^{y_i}\log k
      \right],
    \end{equation}
    which gives us the score function
    \begin{equation}
      S\left(\beta\right) = \sum_{i=1}^n\begin{pmatrix}
        y_i - \exp\left(\beta_0 + \beta_1x_i\right) \\
        x_iy_i - x_i\exp\left(\beta_0 + \beta_1x_i\right)
      \end{pmatrix}.
      \label{eqn:p1_score_function}
    \end{equation}
    We can estimate $\beta$ by solving for
    $S\left(\hat{\beta}\right) = \mathbf{0}$, numerically.

    We can estimate the variance of the estimates from the Fisher information,
    \begin{align}
      \operatorname{Var}\left(
      \hat{\beta}
      \right)
      &\approx
        I_n\left(\hat{\beta}\right)^{-1} \label{eqn:p2_beta_hat_variance}\\
      &= \left(
        \sum_{i=1}^n\begin{pmatrix}
          \exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right) &
          x_i\exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right) \\
          x_i\exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right) &
          x_i^2\exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right)
        \end{pmatrix} \right)^{-1}  \nonumber\\
      &= \frac{1}{
        \left(\sum_{i=1}^n \hat{\mu}_i\right)\left(\sum_{i=1}^n x_i^2\hat{\mu}_i\right)
        - \left(\sum_{i=1}^n x_i\hat{\mu}_i\right)^2}
        \begin{pmatrix}
          \sum_{i=1}^n x_i^2\hat{\mu}_i &
          -\sum_{i=1}^n x_i\hat{\mu}_i \\
          -\sum_{i=1}^n x_i\hat{\mu}_i &
          \sum_{i=1}^n \hat{\mu}_i
        \end{pmatrix}, \nonumber
    \end{align}
    where $\hat{\mu}_i = \exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right)$.
    
    \subsection*{Quasi-likelihood}

    In a quasi-likelihood model, we specify the mean and variance as
    \begin{align}
      \mathbb{E}\left[
      Y_i
      \mid x_i
      \right]
      &= \mu_i = \exp\left(\beta_0 + \beta_1x_i\right) \nonumber\\
      \operatorname{Var}\left(Y_i \mid x_i\right)
      &= \alpha \mu_i
      \label{eqn:p1_y_quasi_mean_variance}
    \end{align}
    From Equation \ref{eqn:p1_y_mean_variance}, we see that this is not quite
    correct, still, but it is closer to the real model than the Poisson model.

    Then, by Equation 2.30 of Wakefield's \emph{Bayesian and Frequentist
      Regression Methods} our estimating function is
    \begin{align}
      U\left(\beta\right)
      &= D^\intercal V^{-1}\left(y - \mu\right)/\alpha \nonumber\\
      &= \sum_{i=1}^n\begin{pmatrix}
        \exp\left(\beta_0 + \beta_1x_i\right) \\
        x_i\exp\left(\beta_0 + \beta_1x_i\right)
      \end{pmatrix}
      \frac{y_i - \exp\left(\beta_0 + \beta_1x_i\right)}{\alpha\exp\left(\beta_0 + \beta_1x_i\right)}
      \nonumber\\
      &= \frac{1}{\alpha}\sum_{i=1}^n\begin{pmatrix}
        y_i - \exp\left(\beta_0 + \beta_1x_i\right) \\
        x_iy_i - x_i\exp\left(\beta_0 + \beta_1x_i\right)
      \end{pmatrix} = \frac{1}{\alpha}S\left(\beta\right)
      \label{eqn:p1_quasi_likelihood_score_function}
    \end{align}
    from Equation \ref{eqn:p1_score_function}. Thus, the maximum
    quasi-likelihood estimate will be the same as the maximum likelihood
    estimate from the Poisson model.

    Having solved for $\hat{\beta}$, we have
    \begin{equation}
      \hat{\mu} = \exp\left(\hat{\beta}_0 + \hat{\beta}_1x_i\right).
      \label{eqn:p1_mu_hat}
    \end{equation}

    by Equation 2.31 of Wakefield's \emph{Bayesian and Frequentist Regression
      Methods}, we can then compute
    \begin{equation}
      \hat{\alpha}_n
      = \frac{1}{n-2}\sum_{i=1}^n \frac{\left(y_i - \hat{\mu}_i\right)^2}{\hat{\mu}_i}
      \label{eqn:p1_alpha_hat}
    \end{equation}

    Then, the variance of our estimates is
    \begin{align}
      \operatorname{Var}\left(
      \hat{\beta}
      \right)
      &\approx
      \hat{\alpha}_n\left(\hat{D}^\intercal \hat{V}^{-1} \hat{D}\right)^{-1} \nonumber\\
      &= \hat{\alpha}_n\left(\sum_{i=1}^n
        \begin{pmatrix}
          \hat{\mu_i} & x_i\hat{\mu_i} \\
          x_i\hat{\mu_i} & x_i^2\hat{\mu_i}.
        \end{pmatrix}\right)^{-1}
        \nonumber\\
      &= \hat{\alpha}_nI_n\left(\hat{\beta}\right)^{-1}
    \end{align}
    from Equation \ref{eqn:p2_beta_hat_variance}.

    \subsection*{Sandwich Estimation}

    In sandwich estimation, we only need to specify an estimating function
    $G\left(\beta\right)$. Then, we can apply Equation 2.43 of Wakefield's
    \emph{Bayesian and Frequentist Regression Methods} to compute the variance
    of our estimates:
    \begin{align}
      \operatorname{Var}\left(\hat{\beta}\right)
      &= \hat{A}^{-1}\hat{B}\left(\hat{A}^{-1}\right)^\intercal \nonumber\\
      \hat{A} &= -\frac{\partial}{\partial\beta}G\left(\hat{\beta}\right) \nonumber\\
      \hat{B} &= G\left(\hat{\beta}\right)G\left(\hat{\beta}\right)^\intercal. \nonumber
    \end{align}

    We can reuse the score function from the quasi-likelihood estimate in
    Equation \ref{eqn:p1_quasi_likelihood_score_function}. Thus, our estimate
    for $\hat{\beta}$ will remain the same.

    From Equation \ref{eqn:p1_y_quasi_mean_variance}, we have that
    \begin{equation}
      \hat{A} = \hat{D}\hat{V}^{-1}\hat{D} = \frac{I_n\left(\hat{\beta}\right)}{\hat{\alpha}_n}.
    \end{equation}

    From Equation \ref{eqn:p1_quasi_likelihood_score_function}, we have that
    \begin{align}
      \hat{B}
      &= \hat{D}\begin{pmatrix}
        \frac{\left(y_1 - \hat{\mu}_1\right)^2}{\hat{\alpha}_n^2\hat{\mu}_1^2} \\
        \vdots \\
        \frac{\left(y_n - \hat{\mu}_n\right)^2}{\hat{\alpha}_n^2\hat{\mu}_n^2}
      \end{pmatrix}\hat{D}^\intercal =
      \sum_{i=1}^n \frac{\left(y_i - \hat{\mu}_i\right)^2}{\hat{\alpha}_n^2\hat{\mu}_i^2}
      \begin{pmatrix}
        \hat{\mu}_i^2 & x_i\hat{\mu}_i^2 \\
        x_i\hat{\mu}_i^2 & x_i^2\hat{\mu}_i^2
      \end{pmatrix} \nonumber\\
      &= \frac{1}{\hat{\alpha}_n^2}\sum_{i=1}^n\left(y_i-\hat{\mu}_i\right)^2
        \begin{pmatrix}
          1 & x_i\\
          x_i & x_i^2
        \end{pmatrix}.
    \end{align}
    
  \end{description}
\end{enumerate}
\end{document}
