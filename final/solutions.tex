\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{subcaption}

\title{Final: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle

Consider the failure time data in Table \ref{tab:failure_time_data}.

\begin{enumerate}
\item We describe a simple model for these data. Let $p$ ($0 < p < 1$) denote
  the weekly failure probability, i.e., the probability of failure during any
  week, and $T$ the random variable describing the week at which failure
  occurred. Then $T$ may be modeled as a geometric random variable:
  \begin{equation}
    \mathbb{P}\left(T = t \mid p\right)
    = \begin{cases}
      p\left(1-p\right)^{t-1}, &t=1,2,\ldots; \\
      0,&\text{otherwise}.      
    \end{cases}
    \label{eqn:p1_model}
  \end{equation}

  Let $Y_t$ represent the number of components that fail in week $t$,
  $t = 1,2,\ldots,N$, and $Y_{N+1}$ the number of components that have not failed
  by week $N$.

  \begin{enumerate}
  \item Show that the likelihood function is
    \begin{equation}
      L\left(p\right) =
      \left[\left(1 - p\right)^N\right]^{Y_{N+1}}
      \prod_{t=1}^N\left[
        p\left(1 - p\right)^{t-1}
      \right]^{Y_t}.
      \label{eqn:p1_likelihood}      
    \end{equation}
    \begin{description}
    \item[Solution:] An individual component's failure week has distribution
      $\operatorname{Geometric}\left(p\right)$. The probability that a single
      component fails in week $t$ is the probability that it survived $t - 1$
      weeks and failed on week $t$, which is $p\left(1 - p\right)^{t-1}$. There
      are $Y_t$ such components, which gives us the factors for
      $t = 1,2,\ldots,N$.

      The probability that a component fails at a later date is
      \[
        \left(1 - p\right)^N\sum_{k=1}^\infty p\left(1-p\right)^{k-1}
        =
        \left(1 - p\right)^N\frac{p}{1 - \left(1 - p\right)}
        =
        \left(1 - p\right)^N,
      \]
      which gives us the remaining factor. There are $Y_{N+1}$ remaining
      components, so
      \[
        L\left(p\right) =
        \left\{
          \prod_{t=1}^N \left[
            p\left(1 - p \right)^{t-1}
          \right]^{Y_t}
        \right\}
        \times
        \left[\left(1-p\right)^N\right]^{Y_{N+1}}.
      \]
    \end{description}
  \item Find an expression for the MLE $\hat{p}$.
    \begin{description}
    \item[Solution:] 
      The score function is
      \begin{align}
        S\left(p\right)
        &= \frac{\partial}{\partial p}\log L\left(p\right) \nonumber\\
        &= \frac{\partial}{\partial p}\left[
          NY_{N + 1}\log\left(1 - p\right)
          + \sum_{t=1}^N Y_t\left(
          \log p + \left(t - 1\right)
          \log\left(1 - p\right)
          \right)
          \right]\nonumber\\
        &= -\frac{NY_{N+1}}{1 - p}
          + \sum_{t=1}^N Y_t\left(
          \frac{1}{p}
          -
          \frac{t - 1}{1 - p}
          \right)
        = -\frac{NY_{N+1}}{1 - p}
          + \sum_{t=1}^N Y_t
          \frac{1-pt}{p\left(1 - p\right)}.
          \label{eqn:p1_score}
      \end{align}

      Solving for $S\left(\hat{p}\right) = 0$, we find the MLE:
      \begin{equation}
        \hat{p}\left(
          NY_{N+1} + \sum_{t=1}^{N}tY_t
        \right) = \sum_{t=1}^N Y_t
        \implies
        \boxed{
          \hat{p} = \frac{\sum_{t=1}^N Y_t}
          {NY_{N+1} + \sum_{t=1}^{N}tY_t}.
        }
        \label{eqn:p1_mle}
      \end{equation}
    \end{description}
  \item Find the form of the observed information and hence the asymptotic
    variance of the maximum likelihood estimate (MLE).
    \begin{description}
    \item[Solution:] Using Equation \ref{eqn:p1_score}, the expected observed
      information is
      \begin{align}
        I\left(p\right)
        &= \mathbb{E}\left[-\frac{\partial}{\partial p}S\left(p\right) \mid p\right]
          \nonumber\\
        &= \frac{N\mathbb{E}\left[Y_{N+1} \mid p\right]}{(1-p)^2}
          + \sum_{t=1}^N\mathbb{E}\left[Y_t \mid p\right]
          \left(
          \frac{1}{p^2} + \frac{t-1}{\left(1-p\right)^2}
          \right)
          \nonumber \\
        &= n\frac{N\left(1-p\right)^N}{(1-p)^2}
          + np\sum_{t=1}^N
          \left(1 - p\right)^{t-1}
          \left(
          \frac{1}{p^2} + \frac{t-1}{\left(1-p\right)^2}
          \right)
          \nonumber \\
        &= n\left[
          \frac{\left(1-p\right)^N}{(1-p)^2}
          +
          \frac{1 - \left(1 - p\right)^N}{p^2}
          +
          \frac{(1-p) - (1-p)^N}{p(1-p)^2}
          \right] \nonumber \\
        &= \boxed{n\frac{1 - \left(1 - p\right)^N}
          {p^2\left(1 - p\right)},} \label{eqn:p1_fisher_information}
      \end{align}
      where $n = Y_{N+1} + \sum_{t=1}^N Y_t$.

      From Equation \ref{eqn:p1_fisher_information}, the asymptotic variance of
      $\hat{p}$ is
      \begin{equation}
        \operatorname{var}\left(\hat{p}\right)
        \approx
        \hat{\operatorname{var}}\left(\hat{p}\right) = 
        I\left(\hat{p}\right)^{-1} = \frac{1}{n} \times
        \frac{\hat{p}^2\left(1-\hat{p}\right)}
        {1 - \left(1-\hat{p}\right)^N}
        \label{eqn:p1_variance}
      \end{equation}
      by asymptotic normality of the MLE.
    \end{description}
  \item For the data in Table \ref{tab:failure_time_data}, calculate the MLE,
    $\hat{p}$. the variance of $\hat{p}$, and an asymptotic
    95\% confidence interval for $p$.

    \begin{description}
    \item[Solution:] The MLE can be calculated with Equation \ref{eqn:p1_mle} to
      be $\boxed{\hat{p} = 0.354717.}$ The variance can be found with Equation
      \ref{eqn:p1_variance} to be
      $\boxed{\hat{\operatorname{var}}\left(\hat{p}\right) = 0.00016828.}$

      If $\Phi$ is the cumulative distribution function for a standard normal,
      we can use asymptotic normality to find the 95\% confidence interval as
      \begin{equation*}
        \left[
          \hat{p} +
          \Phi^{-1}\left(0.025\right)\sqrt{\hat{\operatorname{var}}\left(\hat{p}\right)},
          \hat{p} +
          \Phi^{-1}\left(0.975\right)\sqrt{\hat{\operatorname{var}}\left(\hat{p}\right)}
        \right]
        =
        \boxed{\left[0.32929,0.38014\right].}
      \end{equation*}
    \end{description}
  \item We now consider a Bayesian analysis. The conjugate prior for $p$ is a
    beta distribution, $\operatorname{Beta}\left(a, b\right)$. State the form of
    the posterior with this choice. Give the form of the posterior mean and
    write as a weighted combination of the MLE and the prior mean.

    \begin{description}
    \item[Solution:] By Bayes' rule, we know the posterior density is
      proportional to the likelihood times the prior. From Equation
      \ref{eqn:p1_likelihood}, we'll have
      \begin{align*}
        L\left(p\right) \times \left[
        p^{a-1}\left(1-p\right)^{b-1}
        \right]
        &= p^{a-1}\left(1 - p\right)^{b + NY_{N+1} - 1}
          \prod_{t=1}^N\left[
          p\left(1 - p\right)^{t-1}
          \right]^{Y_t} \\
        &= p^{a + \sum_{t=1}^N Y_t -1}
          \left(1 - p\right)^{b + \sum_{t=1}^N (t-1)Y_t + NY_{N+1} - 1},
      \end{align*}
      whose form we recognize as the integrand of beta function, so the
      posterior also has beta distribution, that is,
      \begin{align}
        p \mid Y_1,Y_2,\ldots,Y_{N+1}
        &\sim \operatorname{Beta}\left(
          a + \sum_{t=1}^N Y_t,
          b + \sum_{t=1}^N (t-1)Y_t + NY_{N+1}
          \right) \nonumber\\
        &= \frac{\Gamma\left(a^\prime + b^\prime\right)}
          {\Gamma\left(a^\prime\right)\Gamma\left(b^\prime\right)}
          p^{a^\prime -1}\left(1 - p\right)^{b^\prime - 1},
      \end{align}
      where $a^\prime = a + \sum_{t=1}^N Y_t$ and
      $b^\prime = b + \sum_{t=1}^N (t-1)Y_t + NY_{N+1}$.

      The posterior mean takes the form
      \begin{align}
        \mathbb{E}\left[
        p \mid Y_1,Y_2,\ldots,Y_{N+1}
        \right]
        &= \frac{a^\prime}{a^\prime + b^\prime}
          \nonumber \\
        &= \frac{a + \sum_{t=1}^N Y_t}
          {a + b + \sum_{t=1}^N tY_t + NY_{N+1}}. \label{eqn:p1_posterior_mean}
      \end{align}

      We have that the prior mean is $p_{\mathrm{prior}} = \frac{a}{a + b}$.
      Equation \ref{eqn:p1_posterior_mean} can be rewritten as
      \begin{equation}
        \boxed{
        \frac{
          \left(a + b\right)
          p_{\mathrm{prior}}
          +
          \left(
            \sum_{t=}^N tY_t + NY_{N+1}
          \right)
          \hat{p}}
          {a + b + \sum_{t=}^N tY_t + NY_{N+1}},}
        \label{eqn:p1_posterior_mean_sum}
      \end{equation}
      so the posterior mean is a convex combination of the prior mean and MLE.
    \end{description}
  \item Suppose we wish to fix the parameters of the prior, $a$ and $b$, so that
    the mean is $\mu$ and the prior standard deviation is $\sigma$. Obtain
    expressions for $a$ and $b$ in terms of $\mu$ and $\sigma^2$.
    \begin{description}
    \item[Solution:] It is well known that the mean and variance of the
      $\operatorname{Beta}\left(a,b\right)$ distribution are $\frac{a}{a+b}$ and
      $\frac{ab}{(a+b)^2(a+b+1)}$, respectively.

      Solving equations
      \begin{align*}
        \frac{a}{a + b}
        &= \mu \\
        \frac{ab}{(a+b)^2(a+b+1)} &= \sigma^2,
      \end{align*}
      we find that
      \begin{align}
        a
        &= \mu\left[
          \frac{\mu\left(1 - \mu\right)}{\sigma^2} - 1
          \right]
          \label{eqn:p1_a} \\
        b
        &=  \left(1 - \mu\right)\left[
          \frac{\mu\left(1 - \mu\right)}{\sigma^2} - 1
          \right].
          \label{eqn:p1_b}
      \end{align}
    \end{description}
  \item For the data in Table \ref{tab:failure_time_data}, assume we wish to
    have a beta prior with $\mu = 0.2$ and $\sigma = 0.08$. State the posterior
    for the prior corresponding to this choice and evaluate the posterior
    mean. Simulate samples from the posterior distribution. Provide a histogram
    representation of the posterior distribution and calculate the 5\%, 50\% and
    95\% points of the posterior distribution.

    \begin{description}
    \item[Solution:] 
    \end{description}
  \end{enumerate}
\end{enumerate}

\begin{table}
  \centering
  \input{failure_time_data.tex}
  \caption{Time until failure for $n = 485$ components, along with average weekly
    temperature.}
  \label{tab:failure_time_data}
\end{table}
\end{document}
