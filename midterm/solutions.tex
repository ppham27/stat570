\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{pdflscape}
\usepackage{subcaption}

\title{Midterm: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle
\begin{enumerate}
\item Consider an situation in which we are interested in the risk of death in
  the first 5 years of life (the under-5 mortality mortality risk, or U5MR) in
  each of $2n$ areas in two consecutive time periods. Consider a hypothetical
  situation in which a malaria prevention intervention is randomized across the
  areas, immediately after the first time periods. Areas indexed by
  $i = 1,\ldots,n$ are control areas, while areas $i = n + 1,\ldots,2n$
  receiving the intervention.

  In each area and each time period alive/dead status of $M_{it}$ children are
  recorded, call the number dead $D_{it}$ for $i = 1,\ldots,2n$, $t = 0, 1$. Let
  \begin{equation}
    Y_{it} = \log\left(\frac{D_{it}/M_{it}}{1 - D_{it}/M_{it}}\right),
    \label{eqn:p1_logit}
  \end{equation}
  denote the logit of the U5MR in area $i$ in period $t$, $i = 1,\ldots,n$,
  $t = 0, 1$.

  Suppose the true model is given by
  \begin{equation}
    Y_{it} = \beta_0 + \alpha_i + \beta_1x_{it} + \epsilon_{it},
  \end{equation}
  where $\alpha_i \sim \mathcal{N}\left(0, \sigma^2_\alpha\right)$ are
  area-specific random effects and
  $\epsilon_{it} \sim \mathcal{N}\left(0, \sigma_\epsilon^2\right)$, represents
  measurement error, with $\alpha_i$ and $\epsilon_{it}$ independent,
  $i = 1,\ldots,2n$, $t = 0, 1$. The covariate $x_{it}$ is an indicator for the
  intervention so that $x_{i0} = 0$ for $i = 1,\ldots,2n$, $x_{i1} = 0$ for
  $i = 1,\ldots,n$, and $x_{i1} = 1$ for $i = n + 1,\ldots,2n$.

  We will consider three models for the child mortality data:  
  \begin{description}
  \item[Follow-up model:]
    $Y_{i1} = \beta_0^\dagger + \beta_1^\dagger x_{i1} + \epsilon_{i1}^\dagger$,
    for $i = 1,\ldots,2n$.
  \item[Change model:]
    $Z_i = Y_{i1} - Y_{i0} = \beta_0^\star + \beta_1^\star x_{i1} +
    \epsilon_i^\star$, for $i = 1,\ldots,2n$.
  \item[Analysis for Covariance (ANCOVA) model:]
    $Y_{i1} = \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} +
    \epsilon_i^\ddagger$, for $i = 1,\ldots,2n$.
  \end{description}
  \begin{enumerate}
  \item Carefully interpret $\beta_1^\dagger$, $\beta_1^\star$ and
    $\beta_1^\ddagger$ in these models, and hence what each of
    $\mathbb{E}\left[\hat{\beta}^\dagger_1\right]$,
    $\mathbb{E}\left[\hat{\beta}^\star_1\right]$, and
    $\mathbb{E}\left[\hat{\beta}_1^\ddagger\right]$ are unbiased estimators of.
    \begin{description}
    \item[Solution:] Let's examine each case.
      \begin{description}
      \item[$\beta_1^\dagger$:] Let
        $Y_{:,1} = \begin{pmatrix} Y_{1,1} & \cdots & Y_{2n,1}
        \end{pmatrix}^\intercal$. Let
        $\beta = \begin{pmatrix} \beta_0 & \beta_1
        \end{pmatrix}^\intercal$. Let $X$ be the $2n \times 2$ matrix with $1$s
        in the first column and $x_{1,1},\ldots,x_{2n,1}$ in the second column.
        We can write $Y_{:,1} = X\beta + \alpha_i + \epsilon_{:,1}$.

        We have that
        \begin{align}
          \hat{\beta}^\dagger
          &= \left(X^\intercal X\right)^{-1}X^\intercal Y_{:,1}
            = \left(X^\intercal X\right)^{-1}X^\intercal\left(
            X\beta + \alpha + \epsilon_{:,1}
            \right)\nonumber\\
          &= \beta + \left(X^\intercal X\right)^{-1}X^\intercal\left(
            \alpha + \epsilon_{:,1}
            \right) \nonumber\\
          &\sim \mathcal{N}\left(\beta,
            \left(\sigma^2_\alpha + \sigma^2_\epsilon\right)\left(X^\intercal X\right)^{-1}
            \right),
            \label{eqn:p1_follow_estimator_dist}
        \end{align}
        so we'll obtain unbiased estimates of $\beta$ with higher variance than
        if we had the correct model.

        So, $\beta_1^\dagger$ is the expected change in the logit of the U5MR
        after applying the treatment.
      \item[$\beta_1^\star$:] We have that
        $Z_i = Y_{i1} - Y_{i0} = \beta_1\left(x_{i1} - x_{i0}\right) +
        \epsilon_{i1} - \epsilon_{i0} = \beta_1x_{i1} + \left(\epsilon_{i1} -
          \epsilon_{i0}\right)$.

        Solving for $\hat{\beta}^\star$, we find
        \begin{align}
          \hat{\beta}^\star
          &= \left(X^\intercal X\right)^{-1}X^\intercal Z_i
            = \left(X^\intercal X\right)^{-1}X^\intercal \left(
            X\begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix}
          + \left(\epsilon_{:,1} - \epsilon_{:,0}\right)
          \right) \nonumber\\
          &= \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix} + \left(X^\intercal X\right)^{-1}X^\intercal\left(\epsilon_{:,1} - \epsilon_{:,0}\right) \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix},
          2\sigma^2_\epsilon\left(X^\intercal X\right)^{-1}
          \right),
          \label{eqn:p1_change_estimator_dist}
        \end{align}
        so $\hat{\beta}_1^\star$ is an unbiased estimate of $\beta_1$.

        Thus, $\beta_1^\star$ is again the expected change in the logit of
        the U5MR after applying the treatment.
      \item[$\beta_1^\ddagger$:] Consider the different ways of writing $Y_{i1}$,
        \begin{align*}
          Y_{i1}
          &= \beta_0 + \alpha_i + \beta_1x_{i1} + \epsilon_{i1} \\
          &= \left(\beta_0 + \alpha_i + \beta_1x_{i0} + \epsilon_{i0}\right) +
            \beta_1x_{i1} + \epsilon_{i1} - \epsilon_{i0} \\
          &= Y_{i0} + \beta_1x_{i1} + \left(\epsilon_{i1} - \epsilon_{i0}\right) \\
          &= \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} + \epsilon_i^\ddagger.
        \end{align*}

        Define $X^\ddagger$ to be the $2n \times 3$ matrix with the first two
        columns being $X$ and third column being $Y_{:,0}$.

        Then, we have that
        \begin{align}
          \begin{pmatrix}
            \hat{\beta}_0^\ddagger \\
            \hat{\beta}_1^\ddagger \\
            \hat{\gamma}            
          \end{pmatrix}
          &= \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}
            \left(X^\ddagger\right)^\intercal Y_{:,1} \nonumber\\
          &= \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}
            \left(X^\ddagger\right)^\intercal\left(
            X^\ddagger \begin{pmatrix}
              0 \\
              \beta_1 \\
              1
            \end{pmatrix} + \epsilon_{:,1} - \epsilon_{:,0}
          \right) \nonumber\\
          &= \begin{pmatrix}
              0 \\
              \beta_1 \\
              1
            \end{pmatrix} + \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}
          \left(X^\ddagger\right)^\intercal\left(\epsilon_{:,1} - \epsilon_{:,0}\right) \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              0 \\
              \beta_1 \\
              1
            \end{pmatrix},
          2\sigma^2_\epsilon\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}          
          \right).
          \label{eqn:p1_ancova_estimator_dist}
        \end{align}

        Again, $\hat{\beta}^\ddagger_1$ is an unbiased estimate of $\beta_1$.
      \end{description}

      All in all, we have that the expected value of the estimates
      \begin{equation}
        \mathbb{E}\left[\hat{\beta}^\dagger_1\right] =
        \mathbb{E}\left[\hat{\beta}^\star_1\right] = 
        \mathbb{E}\left[\hat{\beta}_1^\ddagger\right] =
        \beta_1,
        \label{eqn:p1_expectation}
      \end{equation}
      so $\beta_1^\dagger$, $\beta_1^\star$, $\beta_1^\ddagger$ can all be
      interpreted as the expected change in U5MR after applying the treatment.
    \end{description}
  \item Evaluate $\operatorname{var}\left(\hat{\beta}_1^\dagger\right)$,
    $\operatorname{var}\left(\hat{\beta}_1^\star\right)$, and
    $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)$. Comment on the
    efficiency of the estimators arising from each of the three models.
    \begin{description}
    \item[Solution:] While Equation \ref{eqn:p1_expectation} tells us that the
      expectation of our estimators is the same, the variances are different.

      \begin{description}
      \item[$\hat{\beta}_1^\dagger$:] We can compute the variance from Equation
        \ref{eqn:p1_follow_estimator_dist}. First, we have that
        \begin{align}
          X^\intercal X
          &= \begin{pmatrix}
            2n & \sum_{i=1}^{2n} x_{i1}  \\
            \sum_{i=1}^{2n} x_{i1} & \sum_{i=1}^{2n} x_{i1}^2
          \end{pmatrix} = \begin{pmatrix}
            2n & n  \\
            n & n
          \end{pmatrix} \nonumber\\
          &\implies \left(X^\intercal X\right)^{-1}
            = \frac{1}{n^2}\begin{pmatrix}
              n & -n \\
              -n & 2n
            \end{pmatrix} = \frac{1}{n}\begin{pmatrix}
              1 & -1 \\
              -1 & 2
            \end{pmatrix}. \label{eqn:p1_inverse_gramian}
        \end{align}

        Thus, we find that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\dagger\right)
          = \frac{2}{n}\left(\sigma_\alpha^2 + \sigma_{\epsilon}^2\right).
          \label{eqn:p1_follow_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\star$:] Using Equations
        \ref{eqn:p1_change_estimator_dist} and \ref{eqn:p1_inverse_gramian}, we
        compute that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\star\right)
          = \frac{4}{n}\sigma_{\epsilon}^2.
          \label{eqn:p1_change_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\ddagger$:] We use Equation
        \ref{eqn:p1_ancova_estimator_dist} to compute the variance. First, we
        note that
        \begin{align}
          &\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)
          = 2\sigma_\epsilon^2\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}_{2,2}
            \nonumber\\
          &= 2\sigma_\epsilon^2\left(
            \frac{2n\sum_{i=1}^{2n} Y_{i0}^2 - \left(\sum_{i=1}^{2n}Y_{i0}\right)^2}
            {\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
            \right),
            \label{eqn:p1_ancova_estimator_variance}
        \end{align}
        where
        \begin{align}
          \det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right) =
          ~&n\left(
            2n\sum_{i=1}^{2n} Y_{i0}^2 - \left(\sum_{i=1}^{2n}Y_{i0}\right)^2
            \right) \label{eqn:p1_determinant}\\
          &- n \det\left(\sum_{i=1}^{2n}\begin{pmatrix}
                x_{i1} & x_{i1}Y_{i0} \\
                Y_{i0} & Y_{i0}^2
              \end{pmatrix}\right) \nonumber\\
          &- \left(\sum_{i=1}^{2n}x_{i1}Y_{i0}\right)\det\left(
            \sum_{i=1}^{2n}
            \begin{pmatrix}
              1 & Y_{i0} \\
              x_{i1} & x_{i1}Y_{i0}
            \end{pmatrix}
                  \right).
            \nonumber
        \end{align}
        Note that
        $\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right) \gneq 0$
        since $\left(X^\ddagger\right)^\intercal X^\ddagger$ is positive
        definite. The first term is positive since it is $4n^3$ times the MLE
        estimate for the variance of $Y_{:,0}$.

        For the second term, note that
        \begin{align}
          \det\left(\sum_{i=1}^{2n}
          \begin{pmatrix}
            x_{i1} & x_{i1}Y_{i0} \\
            Y_{i0} & Y_{i0}^2 \end{pmatrix}
                     \right)
                   &= n\sum_{i=1}^{2n}Y_{i0}^2 -
                     \left(\sum_{i=n+1}^{2n}Y_{i0}\right)\left(\sum_{i=1}^{2n}Y_{i0}\right)
          \label{eqn:p1_determinant_second_term}\\
                   &= \left(n\sum_{i=1}^{n}Y_{i0}^2 - \left(\sum_{i=n+1}^{2n}Y_{i0}\right)
                     \left(\sum_{i=1}^{n}Y_{i0}\right)\right)
          \nonumber\\
                   &~~~+ 
                     \left(n\sum_{i=n+1}^{2n}Y_{i0}^2 - \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2\right),
                     \nonumber
        \end{align}
        so the second term is $2n^3$ times an estimator for the variance of
        $Y_{:,0}$.

        For the third term, note that
        \begin{align}
          \det\left(
          \sum_{i=1}^{2n}
          \begin{pmatrix}
            1 & Y_{i0} \\
            x_{i1} & x_{i1}Y_{i0}
          \end{pmatrix}\right)
                   & = 2n\sum_{i=1}^{2n}x_{i1}Y_{i0} - \left(\sum_{i=1}^{2n}x_{i1}\right)
                     \left(\sum_{i=1}^{2n}Y_{i0}\right), \nonumber\\
              &= 2n\sum_{i=n+1}^{2n}Y_{i0} - n\sum_{i=1}^{2n}Y_{i0}.
                \label{eqn:p1_determinant_third_term}
        \end{align}
        so the third term is $4n^2\sum_{i=1}^{2n}x_{i1}Y_{i0}$ times the MLE
        estimate for the covariance of $x_{:,1}$ and $Y_{:,0}$, which should be
        $0$ if the treatment is randomized.

        Therefore, the numerator of Equation
        \ref{eqn:p1_ancova_estimator_variance} is
        \begin{equation}
          \lim_{n\rightarrow\infty}\frac{
            2n\sum_{i=1}^{2n} Y_{i0}^2 - \left(\sum_{i=1}^{2n}Y_{i0}\right)^2}{\left(2n\right)^2}
          = \operatorname{var}\left(Y_{:,0}\right),
        \end{equation}
        and for the denominator, we use Equations
        \ref{eqn:p1_determinant}, \ref{eqn:p1_determinant_second_term},
        \ref{eqn:p1_determinant_third_term} to obtain
        \begin{equation}
          \lim_{n\rightarrow\infty}\frac{
            \det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}{\left(2n\right)^2}
          = \frac{n}{2}\operatorname{var}\left(Y_{:,0}\right),
        \end{equation}
        so
        $\lim_{n\rightarrow\infty}
        \operatorname{var}\left(\hat{\beta}_1^\ddagger\right) =
        \frac{4}{n}\sigma_\epsilon^2$, which is the same as Equation
        \ref{eqn:p1_change_estimator_variance}.
      \end{description}

      Thus, we have that the follow-up model estimates $\beta_1$ most
      efficiently if $\sigma_\alpha^2 < \sigma_\epsilon^2$. In most cases, we'd
      expect that the measurement error is smaller than the random effect, that
      is, $\sigma_\epsilon^2 < \sigma_\alpha^2$, so the change model would
      estimate $\beta_1$ most efficiently in that case. Asymptotically, the
      ANCOVA model is just as efficient as the change model.

      In practice, since we don't have an infinite number of samples of
      $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right) \gneq
      \operatorname{var}\left(\hat{\beta}_1^\star\right)$. To see this, we show
      that
      \begin{equation}
        \frac{1}{n}\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right) \lneq
        \frac{1}{2}\left(2n\sum_{i=1}^{2n} Y_{i0}^2 - \left(\sum_{i=1}^{2n}Y_{i0}\right)^2\right),
        \label{eqn:p1_determinant_inequality}
      \end{equation}
      which would imply that
      $\operatorname{var}\left(\hat{\beta}^\ddagger\right) \gneq
      \frac{4}{n}\sigma_\epsilon^2$ in Equation
      \ref{eqn:p1_ancova_estimator_variance}.

      From Equations \ref{eqn:p1_determinant},
      \ref{eqn:p1_determinant_second_term}, \ref{eqn:p1_determinant_third_term},
      we have that
      \begin{align*}
        \frac{1}{n}\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 - \left(
          \sum_{i=1}^{n}Y_{i0}\right)\left(\sum_{i=n+1}^{2n}Y_{i0}\right) \\
        &~~~- \left(\sum_{i=n+1}^{2n}Y_{i0}\right)\left(
          2\sum_{i=n + 1}^{2n} Y_{i0} - \sum_{i=1}^{2n}Y_{i0}
          \right) \\
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 -
          \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2.
      \end{align*}
      Using this result and substituting, we'll have that Equation
      \ref{eqn:p1_determinant_inequality} is true if and only if
      \begin{align*}
        n\sum_{i=1}^{2n}Y_{i0}^2
        - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 -
        \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2
        \lneq
          n\sum_{i=1}^{2n} Y_{i0}^2 - \frac{1}{2}\left(\sum_{i=1}^{n}Y_{i0}
           + \sum_{i=n+1}^{2n}Y_{i0}
          \right)^2.
      \end{align*}
      With some algebra, this becomes
      \begin{align*}
        0 &\lneq \left(\sum_{i=1}^{n}Y_{i0}\right)^2
        + \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2
        -2\left(\sum_{i=1}^{n}Y_{i0}\right)\left(\sum_{i=n+1}^{2n}Y_{i0}\right) \\
          &= \left(\sum_{i=1}^{n}Y_{i0} - \sum_{i=n+1}^{2n}Y_{i0}\right)^2,
      \end{align*}
      which is almost surely true, so we have proved Equation
      \ref{eqn:p1_determinant_inequality}, which shows that
      $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right) \gneq
      \frac{4}{n}\sigma_\epsilon^2 =
      \operatorname{var}\left(\hat{\beta}_1^\star\right)$, so the ANCOVA model
      is less efficient than the change model in general.
    \end{description}
  \end{enumerate}
\end{enumerate}
\end{document}
