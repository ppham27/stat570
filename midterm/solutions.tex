\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{pdflscape}
\usepackage{subcaption}

\title{Midterm: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle
\begin{enumerate}
\item Consider an situation in which we are interested in the risk of death in
  the first 5 years of life (the under-5 mortality mortality risk, or U5MR) in
  each of $2n$ areas in two consecutive time periods. Consider a hypothetical
  situation in which a malaria prevention intervention is randomized across the
  areas, immediately after the first time periods. Areas indexed by
  $i = 1,\ldots,n$ are control areas, while areas $i = n + 1,\ldots,2n$
  receiving the intervention.

  In each area and each time period alive/dead status of $M_{it}$ children are
  recorded, call the number dead $D_{it}$ for $i = 1,\ldots,2n$, $t = 0, 1$. Let
  \begin{equation}
    Y_{it} = \log\left(\frac{D_{it}/M_{it}}{1 - D_{it}/M_{it}}\right),
    \label{eqn:p1_logit}
  \end{equation}
  denote the logit of the U5MR in area $i$ in period $t$, $i = 1,\ldots,n$,
  $t = 0, 1$.

  Suppose the true model is given by
  \begin{equation}
    Y_{it} = \beta_0 + \alpha_i + \beta_1x_{it} + \epsilon_{it},
  \end{equation}
  where $\alpha_i \sim \mathcal{N}\left(0, \sigma^2_\alpha\right)$ are
  area-specific random effects and
  $\epsilon_{it} \sim \mathcal{N}\left(0, \sigma_\epsilon^2\right)$, represents
  measurement error, with $\alpha_i$ and $\epsilon_{it}$ independent,
  $i = 1,\ldots,2n$, $t = 0, 1$. The covariate $x_{it}$ is an indicator for the
  intervention so that $x_{i0} = 0$ for $i = 1,\ldots,2n$, $x_{i1} = 0$ for
  $i = 1,\ldots,n$, and $x_{i1} = 1$ for $i = n + 1,\ldots,2n$.

  We will consider three models for the child mortality data:  
  \begin{description}
  \item[Follow-up model:]
    $Y_{i1} = \beta_0^\dagger + \beta_1^\dagger x_{i1} + \epsilon_{i1}^\dagger$,
    for $i = 1,\ldots,2n$.
  \item[Change model:]
    $Z_i = Y_{i1} - Y_{i0} = \beta_0^\star + \beta_1^\star x_{i1} +
    \epsilon_i^\star$, for $i = 1,\ldots,2n$.
  \item[Analysis for Covariance (ANCOVA) model:]
    $Y_{i1} = \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} +
    \epsilon_i^\ddagger$, for $i = 1,\ldots,2n$.
  \end{description}
  \begin{enumerate}
  \item Carefully interpret $\beta_1^\dagger$, $\beta_1^\star$ and
    $\beta_1^\ddagger$ in these models, and hence what each of
    $\mathbb{E}\left[\hat{\beta}^\dagger_1\right]$,
    $\mathbb{E}\left[\hat{\beta}^\star_1\right]$, and
    $\mathbb{E}\left[\hat{\beta}_1^\ddagger\right]$ are unbiased estimators of.
    \begin{description}
    \item[Solution:] Let's examine each case.
      \begin{description}
      \item[$\beta_1^\dagger$:] Let
        $Y_{:,1} = \begin{pmatrix} Y_{1,1} & \cdots & Y_{2n,1}
        \end{pmatrix}^\intercal$. Let
        $\beta = \begin{pmatrix} \beta_0 & \beta_1
        \end{pmatrix}^\intercal$. Let $X$ be the $2n \times 2$ matrix with $1$s
        in the first column and $x_{1,1},\ldots,x_{2n,1}$ in the second column.
        We can write $Y_{:,1} = X\beta + \alpha_i + \epsilon_{:,1}$.

        We have that
        \begin{align}
          \hat{\beta}^\dagger
          &= \left(X^\intercal X\right)^{-1}X^\intercal Y_{:,1}
            = \left(X^\intercal X\right)^{-1}X^\intercal\left(
            X\beta + \alpha + \epsilon_{:,1}
            \right)\nonumber\\
          &= \beta + \left(X^\intercal X\right)^{-1}X^\intercal\left(
            \alpha + \epsilon_{:,1}
            \right) \nonumber\\
          &\sim \mathcal{N}\left(\beta,
            \left(\sigma^2_\alpha + \sigma^2_\epsilon\right)\left(X^\intercal X\right)^{-1}
            \right),
            \label{eqn:p1_follow_estimator_dist}
        \end{align}
        so we'll obtain unbiased estimates of $\beta$ with higher variance than
        if we had the correct model.

        So, $\beta_1^\dagger$ is the expected change in the logit of the U5MR
        after applying the treatment.
      \item[$\beta_1^\star$:] We have that
        $Z_i = Y_{i1} - Y_{i0} = \beta_1\left(x_{i1} - x_{i0}\right) +
        \epsilon_{i1} - \epsilon_{i0} = \beta_1x_{i1} + \left(\epsilon_{i1} -
          \epsilon_{i0}\right)$.

        Solving for $\hat{\beta}^\star$, we find
        \begin{align}
          \hat{\beta}^\star
          &= \left(X^\intercal X\right)^{-1}X^\intercal Z_i
            = \left(X^\intercal X\right)^{-1}X^\intercal \left(
            X\begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix}
          + \left(\epsilon_{:,1} - \epsilon_{:,0}\right)
          \right) \nonumber\\
          &= \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix} + \left(X^\intercal X\right)^{-1}X^\intercal\left(\epsilon_{:,1} - \epsilon_{:,0}\right) \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix},
          2\sigma^2_\epsilon\left(X^\intercal X\right)^{-1}
          \right),
          \label{eqn:p1_change_estimator_dist}
        \end{align}
        so $\hat{\beta}_1^\star$ is an unbiased estimate of $\beta_1$.

        Thus, $\beta_1^\star$ is again the expected change in the logit of
        the U5MR after applying the treatment.
      \item[$\beta_1^\ddagger$:] Consider the different ways of writing $Y_{i1}$,
        \begin{align}
          Y_{i1}
          &= \beta_0 + \alpha_i + \beta_1x_{i1} + \epsilon_{i1} \nonumber\\
          &= \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} + \epsilon_i^\ddagger \nonumber\\
          &= \beta_0^\ddagger + \beta_1^\ddagger x_{i1} +
            \gamma \left(\beta_0 + \alpha_i + \epsilon_{i0}\right) + 
            \epsilon_i^\ddagger \label{eqn:p1_ancova_model_algebra}
        \end{align}
        
        Define $X^\ddagger$ to be the $2n \times 3$ matrix with the first two
        columns being $X$ and third column being $Y_{:,0}$.

        Then, we have that
        \begin{align}
          \begin{pmatrix}
            \hat{\beta}_0^\ddagger \\
            \hat{\beta}_1^\ddagger \\
            \hat{\gamma}            
          \end{pmatrix}
          &= \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}
            \left(X^\ddagger\right)^\intercal Y_{:,1}.
          \label{eqn:p1_ancova_estimator}
        \end{align}

        From Equation \ref{eqn:p1_ancova_model_algebra}, note that
        \begin{align*}
          Y_{i1} - \gamma Y_{i0}          
          &=  \left(1 - \gamma\right)\beta_0 + \beta_1 x_{i1} +
            \left(1 - \gamma\right)\alpha_i - \gamma\epsilon_{i0} + \epsilon_{i1} \\
          &= \beta_0^\ddagger + \beta_1^\ddagger X_{i1} + \epsilon_{i}^\ddagger.
        \end{align*}

        We can estimate $\gamma$ with Equation \ref{eqn:p1_gamma_hat}. Given
        $\hat{\gamma}$, the least squares estimate for $\beta^\ddagger$ is
        \begin{align}
          \begin{pmatrix}
            \hat{\beta}_0^\ddagger \\
            \hat{\beta}_1^\ddagger \\
          \end{pmatrix}
          \mid \hat{\gamma}
          &= \left(X^\intercal X\right)^{-1}X^\intercal\left(
            Y_{:,1} - \hat{\gamma}Y_{:,0}
            \right) \label{eqn:p1_ancova_estimator_dist}\\
          &= \begin{pmatrix}
            \left(1 - \hat{\gamma}\right)\beta_0 \\
            \beta_1
          \end{pmatrix} +
          \left(X^\intercal X\right)^{-1}X^\intercal
          \left(
          \left(1 -\hat{\gamma}\right)\alpha_i +
          \hat{\gamma}\epsilon_{i0} +
          \epsilon_{i1}
          \right)
          \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              \left(1 - \hat{\gamma}\right)\beta_0 \\
              \beta_1
            \end{pmatrix},
          \left(
          \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2
          + \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
          \right)
          \left(X^\intercal X\right)^{-1}
            \right).\nonumber
        \end{align}
        
        Regardless of $\hat{\gamma}$, $\hat{\beta}^\ddagger_1$ is an unbiased
        estimate of $\beta_1$, for
        \[
          \mathbb{E}\left[\hat{\beta}_1^\ddagger\right] =
          \mathbb{E}_{\hat{\gamma}}\left[
            \mathbb{E}\left[\hat{\beta}_1^\ddagger \mid \hat{\gamma}\right]
          \right]
          \mathbb{E}_{\hat{\gamma}}\left[\beta_1\right] = \beta_1
        \]
      by law of total expectation.
      \end{description}

      All in all, we have that the expected value of the estimates
      \begin{equation}
        \mathbb{E}\left[\hat{\beta}^\dagger_1\right] =
        \mathbb{E}\left[\hat{\beta}^\star_1\right] = 
        \mathbb{E}\left[\hat{\beta}_1^\ddagger\right] =
        \beta_1,
        \label{eqn:p1_expectation}
      \end{equation}
      so $\beta_1^\dagger$, $\beta_1^\star$, $\beta_1^\ddagger$ can all be
      interpreted as the expected change in U5MR after applying the treatment.
    \end{description}
  \item Evaluate $\operatorname{var}\left(\hat{\beta}_1^\dagger\right)$,
    $\operatorname{var}\left(\hat{\beta}_1^\star\right)$, and
    $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)$. Comment on the
    efficiency of the estimators arising from each of the three models.
    \label{part:p1_variance}
    \begin{description}
    \item[Solution:] While Equation \ref{eqn:p1_expectation} tells us that the
      expectation of our estimators is the same, the variances are different.

      \begin{description}
      \item[$\hat{\beta}_1^\dagger$:] We can compute the variance from Equation
        \ref{eqn:p1_follow_estimator_dist}. First, we have that
        \begin{align}
          X^\intercal X
          &= \begin{pmatrix}
            2n & \sum_{i=1}^{2n} x_{i1}  \\
            \sum_{i=1}^{2n} x_{i1} & \sum_{i=1}^{2n} x_{i1}^2
          \end{pmatrix} = \begin{pmatrix}
            2n & n  \\
            n & n
          \end{pmatrix} \nonumber\\
          &\implies \left(X^\intercal X\right)^{-1}
            = \frac{1}{n^2}\begin{pmatrix}
              n & -n \\
              -n & 2n
            \end{pmatrix} = \frac{1}{n}\begin{pmatrix}
              1 & -1 \\
              -1 & 2
            \end{pmatrix}. \label{eqn:p1_inverse_gramian}
        \end{align}

        Thus, we find that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\dagger\right)
          = \frac{2}{n}\left(\sigma_\alpha^2 + \sigma_{\epsilon}^2\right).
          \label{eqn:p1_follow_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\star$:] Using Equations
        \ref{eqn:p1_change_estimator_dist} and \ref{eqn:p1_inverse_gramian}, we
        compute that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\star\right)
          = \frac{4}{n}\sigma_{\epsilon}^2.
          \label{eqn:p1_change_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\ddagger$:] We use Equation
        \ref{eqn:p1_ancova_estimator_dist} to compute the variance conditional
        in terms of $\hat{\gamma}$. First, we note that
        \begin{align}
          &\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)
            = \left(
            \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2 +
            \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
            \right)\left(X^\intercal X\right)^{-1}_{22} \nonumber\\
          &= \frac{2}{n}\left(
            \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2 +
            \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
            \right) \nonumber\\
          &= \frac{2}{n}\left(
            \hat{\gamma}^2\left(
            \sigma_\alpha^2 + \sigma_\epsilon^2
            \right)
            - 2\hat{\gamma}\sigma_\alpha^2
            + \sigma_\alpha^2 + \sigma_\epsilon^2
            \right).
            \label{eqn:p1_ancova_estimator_variance}
        \end{align}      
      \end{description}
      From Equations \ref{eqn:p1_follow_estimator_variance} and
      \ref{eqn:p1_change_estimator_variance}, whether the follow-up model or
      change model estimates $\beta_1$ more efficiently depends on whether the
      variance of the random effect is larger than the random effect of the
      measurement error. When the variance of the random effect is larger
      ($\sigma^2_\alpha > \sigma^2_\epsilon$),
      $\operatorname{var}\left(\hat{\beta}_1^\star\right) <
      \operatorname{var}\left(\hat{\beta}_1^\dagger\right)$, so the change model
      is more efficient. Otherwise if $\sigma^2_\alpha < \sigma^2_\epsilon$, the
      follow-up model is more efficient.

      The ANCOVA model is more interesting. From Equation
      \ref{eqn:p1_ancova_estimator_variance}, when $\hat{\gamma} = 0$,
      efficiency is the same as the follow-up model, and when
      $\hat{\gamma} = 1$, efficiency is the same as the change model.
      $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)$ is a strictly
      convex function of $\hat{\gamma}$ which is minimized at
      \begin{equation}
        \hat{\gamma}^* = \frac{\sigma_\alpha^2}{\sigma_\alpha^2 + \sigma_\epsilon^2}.
        \label{eqn:p1_gamma_hat_optimal}
      \end{equation}
      By the Gauss-Markov theorem that states that the ordinary least squares
      estimate gives the lowest variance estimate for unbiased estimators, we
      must have that $\hat{\gamma} = \hat{\gamma}^*$, so
      \begin{align}
        \operatorname{var}\left(\hat{\beta}_1^\ddagger\right)
        &=
        \frac{2}{n}\left(
          \left(1 - \hat{\gamma}^*\right)^2\sigma_\alpha^2 +
          \left(\hat{\gamma}^*\right)^2\sigma_\epsilon^2 + \sigma_\epsilon^2
          \right) \nonumber \\
        &= \frac{2}{n}\left(
          \frac{\sigma_\alpha^2\sigma_\epsilon^2}{\sigma_\alpha^2 + \sigma_\epsilon^2}
          +
          \sigma_\epsilon^2
          \right) 
        \lneq \frac{2}{n}\left(
          \max\left(\sigma_\alpha^2, \sigma_\epsilon^2\right) + \sigma_\epsilon^2
          \right),
          \label{eqn:p1_ancova_estimaor_variance_final}
      \end{align}            
      which results in $\hat{\beta}_1^\ddagger$ being a more efficient estimator
      than both $\hat{\beta}_1^\dagger$ and $\hat{\beta}_1^\star$. The behavior
      of $\hat{\gamma}$ will be investigated more fully in the next two parts.
    \end{description}
  \item Obtain an expression for $\hat{\gamma}$, in as simple a form as you can
    find.
    \begin{description}
    \item[Solution:] From Equation \ref{eqn:p1_ancova_estimator_dist}, we have that
      \begin{align}
        \hat{\gamma}
        &= \left(\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}\left(X^\ddagger\right)^\intercal Y_{:,1}\right)_3 \label{eqn:p1_gamma_hat}\\
        &= \sum_{k=1}^3 \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}_{3k}
          \left(\left(X^\ddagger\right)^\intercal Y_{:,1}\right)_{k} \nonumber\\
        &= 
          -n\frac{\sum_{i=1}^n Y_{i0}}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=1}^{2n}Y_{i1} +
          n\frac{\sum_{i=1}^n Y_{i0} - \sum_{i=n+1}^{2n}Y_{i0}}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=n+1}^{2n}Y_{i1}
          \nonumber\\
        &~~~+ \frac{n^2}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=1}^{2n}Y_{i0}Y_{i1} \nonumber\\
        &= \frac{n}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \left(n\sum_{i=1}^{2n}Y_{i0}Y_{i1}
          -
          \sum_{i=1}^n Y_{i0}\sum_{i=1}^n Y_{i1}
          -
          \sum_{i=n+1}^{2n} Y_{i0}\sum_{i=n+1}^{2n} Y_{i1}
          \right),
          \nonumber
      \end{align}
      where
      $\left. n\middle/\det\left(\left(X^\ddagger\right)^\intercal
          X^\ddagger\right)\right.$ can be obtained from Equation
      \ref{eqn:p1_determinant_expanded}. One can also write Equation
      \ref{eqn:p1_gamma_hat} in terms of empirical variance estimates as in
      Equation \ref{eqn:p1_gamma_hat_variance}.

      \begin{align}
        \frac{1}{n}\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 - \left(
          \sum_{i=1}^{n}Y_{i0}\right)\left(\sum_{i=n+1}^{2n}Y_{i0}\right) \nonumber\\
        &~~~- \left(\sum_{i=n+1}^{2n}Y_{i0}\right)\left(
          2\sum_{i=n + 1}^{2n} Y_{i0} - \sum_{i=1}^{2n}Y_{i0}
          \right) \nonumber\\
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 -
          \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2.
          \label{eqn:p1_determinant_expanded}
      \end{align}

    \end{description}
  \item On the basis of the previous question, or otherwise, give intuitive
    explanations for the efficiency results in Part \ref{part:p1_variance}.
    \begin{description}
    \item[Solution:] Denote the MLE estimates of the covariance between $Y_{i0}$
      and $Y_{i1}$ without and with the intervention by
      \begin{align}
        \hat{\operatorname{cov}}\left(
        Y_{i0}, Y_{i1} \mid x_{i1} = 0
        \right)
        &= \frac{1}{n}\sum_{i=1}^{n} Y_{i0}Y_{i1}
        - \left(\frac{1}{n}\sum_{i=1}^{n} Y_{i0}\right)
          \left(\frac{1}{n}\sum_{i=1}^{n} Y_{i1}\right) \label{eqn:p1_conditional_covariance}\\
        \hat{\operatorname{cov}}\left(
        Y_{i0}, Y_{i1} \mid x_{i1} = 1
        \right)
        &= \frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}Y_{i1}
        - \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}\right)
          \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i1}\right),
          \nonumber
      \end{align}
      respectively. Similarly, we can denote the MLE of the variances of
      $Y_{i0}$ without and with the intervention by
      \begin{align}
        \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 0\right)
        &= \frac{1}{n}\sum_{i=1}^n Y_{i0}^2 - \left(\frac{1}{n}\sum_{i=1}^n Y_{i0}\right)^2
        \label{eqn:p1_conditional_variance}\\
        \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 1\right)
        &= \frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}^2 -
          \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}\right)^2,
          \nonumber
      \end{align}
      respectively. Substituting Equations \ref{eqn:p1_conditional_covariance}
      and \ref{eqn:p1_conditional_variance} into the numerator and denominator
      of Equation \ref{eqn:p1_gamma_hat}, respectively, we can write
      \begin{equation}
        \hat{\gamma} = 
        \frac{\hat{\operatorname{cov}}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 0\right) +
          \hat{\operatorname{cov}}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 1\right)}
        {\hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 0\right) +
          \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 1\right)},
        \label{eqn:p1_gamma_hat_variance}
      \end{equation}
      so we can interpret $\gamma$ as the overall autocorrelation between
      $Y_{i0}$ and $Y_{i1}$. Based on the true model, we can compute
      \begin{align*}
        \operatorname{var}\left(Y_{i0}\right)
        &= \operatorname{var}\left(\epsilon_{i0}\right) + \operatorname{var}\left(\alpha_i\right)
        = \sigma_\epsilon^2 + \sigma_\alpha^2 \\
        \operatorname{cov}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 0\right) &=
        \operatorname{cov}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 1\right) \\
        &= \mathbb{E}\left[\left(\alpha_i + \epsilon_{i0}\right)\left(
          \alpha_i + \epsilon_{i1}
          \right)\right] \\
        &= \operatorname{var}\left(\alpha_i\right) = \sigma^2_\alpha,
      \end{align*}
      so the expected value of $\hat{\gamma}$ is
      \begin{equation}
        \mathbb{E}\left[\hat{\gamma}\right] \approx \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_\epsilon}
        \label{eqn:p1_gamma_hat_expected}     
      \end{equation}
      for large $n$ by Stutsky's theorem, which is the value in Equation
      \ref{eqn:p1_gamma_hat_optimal} that minimizes the variance, so Equation
      \ref{eqn:p1_gamma_hat_variance} agrees with our result in Equation
      \ref{eqn:p1_ancova_estimator_variance_final}.

      Morever, if we don't know $\sigma_\alpha^2$ and $\sigma_\epsilon^2$ a
      priori and use MLE estimates in place, $\hat{\gamma}$ is the minimizer of
      the estimated
    \end{description}      
  \end{enumerate}
\end{enumerate}
\end{document}
