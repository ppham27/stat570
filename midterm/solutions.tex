\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{pdflscape}
\usepackage{subcaption}

\title{Midterm: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle
\begin{enumerate}
\item Consider an situation in which we are interested in the risk of death in
  the first 5 years of life (the under-5 mortality mortality risk, or U5MR) in
  each of $2n$ areas in two consecutive time periods. Consider a hypothetical
  situation in which a malaria prevention intervention is randomized across the
  areas, immediately after the first time periods. Areas indexed by
  $i = 1,\ldots,n$ are control areas, while areas $i = n + 1,\ldots,2n$
  receiving the intervention.

  In each area and each time period alive/dead status of $M_{it}$ children are
  recorded, call the number dead $D_{it}$ for $i = 1,\ldots,2n$, $t = 0, 1$. Let
  \begin{equation}
    Y_{it} = \log\left(\frac{D_{it}/M_{it}}{1 - D_{it}/M_{it}}\right),
    \label{eqn:p1_logit}
  \end{equation}
  denote the logit of the U5MR in area $i$ in period $t$, $i = 1,\ldots,n$,
  $t = 0, 1$.

  Suppose the true model is given by
  \begin{equation}
    Y_{it} = \beta_0 + \alpha_i + \beta_1x_{it} + \epsilon_{it},
  \end{equation}
  where $\alpha_i \sim \mathcal{N}\left(0, \sigma^2_\alpha\right)$ are
  area-specific random effects and
  $\epsilon_{it} \sim \mathcal{N}\left(0, \sigma_\epsilon^2\right)$, represents
  measurement error, with $\alpha_i$ and $\epsilon_{it}$ independent,
  $i = 1,\ldots,2n$, $t = 0, 1$. The covariate $x_{it}$ is an indicator for the
  intervention so that $x_{i0} = 0$ for $i = 1,\ldots,2n$, $x_{i1} = 0$ for
  $i = 1,\ldots,n$, and $x_{i1} = 1$ for $i = n + 1,\ldots,2n$.

  We will consider three models for the child mortality data:  
  \begin{description}
  \item[Follow-up model:]
    $Y_{i1} = \beta_0^\dagger + \beta_1^\dagger x_{i1} + \epsilon_{i1}^\dagger$,
    for $i = 1,\ldots,2n$.
  \item[Change model:]
    $Z_i = Y_{i1} - Y_{i0} = \beta_0^\star + \beta_1^\star x_{i1} +
    \epsilon_i^\star$, for $i = 1,\ldots,2n$.
  \item[Analysis for Covariance (ANCOVA) model:]
    $Y_{i1} = \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} +
    \epsilon_i^\ddagger$, for $i = 1,\ldots,2n$.
  \end{description}
  \begin{enumerate}
  \item Carefully interpret $\beta_1^\dagger$, $\beta_1^\star$ and
    $\beta_1^\ddagger$ in these models, and hence what each of
    $\mathbb{E}\left[\hat{\beta}^\dagger_1\right]$,
    $\mathbb{E}\left[\hat{\beta}^\star_1\right]$, and
    $\mathbb{E}\left[\hat{\beta}_1^\ddagger\right]$ are unbiased estimators of.
    \begin{description}
    \item[Solution:] Let's examine each case.
      \begin{description}
      \item[$\beta_1^\dagger$:] Let
        $Y_{:,1} = \begin{pmatrix} Y_{1,1} & \cdots & Y_{2n,1}
        \end{pmatrix}^\intercal$. Let
        $\beta = \begin{pmatrix} \beta_0 & \beta_1
        \end{pmatrix}^\intercal$. Let $X$ be the $2n \times 2$ matrix with $1$s
        in the first column and $x_{1,1},\ldots,x_{2n,1}$ in the second column.
        We can write $Y_{:,1} = X\beta + \alpha_i + \epsilon_{:,1}$.

        We have that
        \begin{align}
          \hat{\beta}^\dagger
          &= \left(X^\intercal X\right)^{-1}X^\intercal Y_{:,1}
            = \left(X^\intercal X\right)^{-1}X^\intercal\left(
            X\beta + \alpha + \epsilon_{:,1}
            \right)\nonumber\\
          &= \beta + \left(X^\intercal X\right)^{-1}X^\intercal\left(
            \alpha + \epsilon_{:,1}
            \right) \nonumber\\
          &\sim \mathcal{N}\left(\beta,
            \left(\sigma^2_\alpha + \sigma^2_\epsilon\right)\left(X^\intercal X\right)^{-1}
            \right),
            \label{eqn:p1_follow_estimator_dist}
        \end{align}
        so we'll obtain unbiased estimates of $\beta$ with higher variance than
        if we had the correct model.

        So, $\beta_1^\dagger$ is the expected change in the logit of the U5MR
        after applying the treatment.
      \item[$\beta_1^\star$:] We have that
        $Z_i = Y_{i1} - Y_{i0} = \beta_1\left(x_{i1} - x_{i0}\right) +
        \epsilon_{i1} - \epsilon_{i0} = \beta_1x_{i1} + \left(\epsilon_{i1} -
          \epsilon_{i0}\right)$.

        Solving for $\hat{\beta}^\star$, we find
        \begin{align}
          \hat{\beta}^\star
          &= \left(X^\intercal X\right)^{-1}X^\intercal Z_i
            = \left(X^\intercal X\right)^{-1}X^\intercal \left(
            X\begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix}
          + \left(\epsilon_{:,1} - \epsilon_{:,0}\right)
          \right) \nonumber\\
          &= \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix} + \left(X^\intercal X\right)^{-1}X^\intercal\left(\epsilon_{:,1} - \epsilon_{:,0}\right) \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              0 \\
              \beta_1
            \end{pmatrix},
          2\sigma^2_\epsilon\left(X^\intercal X\right)^{-1}
          \right),
          \label{eqn:p1_change_estimator_dist}
        \end{align}
        so $\hat{\beta}_1^\star$ is an unbiased estimate of $\beta_1$.

        Thus, $\beta_1^\star$ is again the expected change in the logit of
        the U5MR after applying the treatment.
      \item[$\beta_1^\ddagger$:] Consider the different ways of writing $Y_{i1}$,
        \begin{align}
          Y_{i1}
          &= \beta_0 + \alpha_i + \beta_1x_{i1} + \epsilon_{i1} \nonumber\\
          &= \beta_0^\ddagger + \gamma Y_{i0} + \beta_1^\ddagger x_{i1} + \epsilon_i^\ddagger \nonumber\\
          &= \beta_0^\ddagger + \beta_1^\ddagger x_{i1} +
            \gamma \left(\beta_0 + \alpha_i + \epsilon_{i0}\right) + 
            \epsilon_i^\ddagger \label{eqn:p1_ancova_model_algebra}
        \end{align}
        
        Define $X^\ddagger$ to be the $2n \times 3$ matrix with the first two
        columns being $X$ and third column being $Y_{:,0}$.

        Then, we have that
        \begin{align}
          \begin{pmatrix}
            \hat{\beta}_0^\ddagger \\
            \hat{\beta}_1^\ddagger \\
            \hat{\gamma}            
          \end{pmatrix}
          &= \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}
            \left(X^\ddagger\right)^\intercal Y_{:,1}.
          \label{eqn:p1_ancova_estimator}
        \end{align}

        From Equation \ref{eqn:p1_ancova_model_algebra}, note that
        \begin{align*}
          Y_{i1} - \gamma Y_{i0}          
          &=  \left(1 - \gamma\right)\beta_0 + \beta_1 x_{i1} +
            \left(1 - \gamma\right)\alpha_i - \gamma\epsilon_{i0} + \epsilon_{i1} \\
          &= \beta_0^\ddagger + \beta_1^\ddagger X_{i1} + \epsilon_{i}^\ddagger.
        \end{align*}

        We can estimate $\gamma$ with Equation \ref{eqn:p1_gamma_hat}. Given
        $\hat{\gamma}$, the least squares estimate for $\beta^\ddagger$ is
        \begin{align}
          \begin{pmatrix}
            \hat{\beta}_0^\ddagger \\
            \hat{\beta}_1^\ddagger \\
          \end{pmatrix}
          \mid \hat{\gamma}
          &= \left(X^\intercal X\right)^{-1}X^\intercal\left(
            Y_{:,1} - \hat{\gamma}Y_{:,0}
            \right) \label{eqn:p1_ancova_estimator_dist}\\
          &= \begin{pmatrix}
            \left(1 - \hat{\gamma}\right)\beta_0 \\
            \beta_1
          \end{pmatrix} +
          \left(X^\intercal X\right)^{-1}X^\intercal
          \left(
          \left(1 -\hat{\gamma}\right)\alpha_i +
          \hat{\gamma}\epsilon_{i0} +
          \epsilon_{i1}
          \right)
          \nonumber\\
          &\sim \mathcal{N}\left(
            \begin{pmatrix}
              \left(1 - \hat{\gamma}\right)\beta_0 \\
              \beta_1
            \end{pmatrix},
          \left(
          \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2
          + \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
          \right)
          \left(X^\intercal X\right)^{-1}
            \right).\nonumber
        \end{align}
        
        Regardless of $\hat{\gamma}$, $\hat{\beta}^\ddagger_1$ is an unbiased
        estimate of $\beta_1$, for
        \[
          \mathbb{E}\left[\hat{\beta}_1^\ddagger\right] =
          \mathbb{E}_{\hat{\gamma}}\left[
            \mathbb{E}\left[\hat{\beta}_1^\ddagger \mid \hat{\gamma}\right]
          \right]
          \mathbb{E}_{\hat{\gamma}}\left[\beta_1\right] = \beta_1
        \]
      by law of total expectation.
      \end{description}

      All in all, we have that the expected value of the estimates
      \begin{equation}
        \mathbb{E}\left[\hat{\beta}^\dagger_1\right] =
        \mathbb{E}\left[\hat{\beta}^\star_1\right] = 
        \mathbb{E}\left[\hat{\beta}_1^\ddagger\right] =
        \beta_1,
        \label{eqn:p1_expectation}
      \end{equation}
      so $\beta_1^\dagger$, $\beta_1^\star$, $\beta_1^\ddagger$ can all be
      interpreted as the expected change in U5MR after applying the treatment.
    \end{description}
  \item Evaluate $\operatorname{var}\left(\hat{\beta}_1^\dagger\right)$,
    $\operatorname{var}\left(\hat{\beta}_1^\star\right)$, and
    $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)$. Comment on the
    efficiency of the estimators arising from each of the three models.
    \label{part:p1_variance}
    \begin{description}
    \item[Solution:] While Equation \ref{eqn:p1_expectation} tells us that the
      expectation of our estimators is the same, the variances are different.

      \begin{description}
      \item[$\hat{\beta}_1^\dagger$:] We can compute the variance from Equation
        \ref{eqn:p1_follow_estimator_dist}. First, we have that
        \begin{align}
          X^\intercal X
          &= \begin{pmatrix}
            2n & \sum_{i=1}^{2n} x_{i1}  \\
            \sum_{i=1}^{2n} x_{i1} & \sum_{i=1}^{2n} x_{i1}^2
          \end{pmatrix} = \begin{pmatrix}
            2n & n  \\
            n & n
          \end{pmatrix} \nonumber\\
          &\implies \left(X^\intercal X\right)^{-1}
            = \frac{1}{n^2}\begin{pmatrix}
              n & -n \\
              -n & 2n
            \end{pmatrix} = \frac{1}{n}\begin{pmatrix}
              1 & -1 \\
              -1 & 2
            \end{pmatrix}. \label{eqn:p1_inverse_gramian}
        \end{align}

        Thus, we find that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\dagger\right)
          = \frac{2}{n}\left(\sigma_\alpha^2 + \sigma_{\epsilon}^2\right).
          \label{eqn:p1_follow_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\star$:] Using Equations
        \ref{eqn:p1_change_estimator_dist} and \ref{eqn:p1_inverse_gramian}, we
        compute that
        \begin{equation}
          \operatorname{var}\left(\hat{\beta}_1^\star\right)
          = \frac{4}{n}\sigma_{\epsilon}^2.
          \label{eqn:p1_change_estimator_variance}
        \end{equation}
      \item[$\hat{\beta}_1^\ddagger$:] We use Equation
        \ref{eqn:p1_ancova_estimator_dist} to compute the variance conditional
        in terms of $\hat{\gamma}$. First, we note that
        \begin{align}
          &\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)
            = \left(
            \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2 +
            \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
            \right)\left(X^\intercal X\right)^{-1}_{22} \nonumber\\
          &= \frac{2}{n}\left(
            \left(1 - \hat{\gamma}\right)^2\sigma_\alpha^2 +
            \hat{\gamma}^2\sigma_\epsilon^2 + \sigma_\epsilon^2
            \right) \nonumber\\
          &= \frac{2}{n}\left(
            \hat{\gamma}^2\left(
            \sigma_\alpha^2 + \sigma_\epsilon^2
            \right)
            - 2\hat{\gamma}\sigma_\alpha^2
            + \sigma_\alpha^2 + \sigma_\epsilon^2
            \right).
            \label{eqn:p1_ancova_estimator_variance}
        \end{align}      
      \end{description}
      From Equations \ref{eqn:p1_follow_estimator_variance} and
      \ref{eqn:p1_change_estimator_variance}, whether the follow-up model or
      change model estimates $\beta_1$ more efficiently depends on whether the
      variance of the random effect is larger than the random effect of the
      measurement error. When the variance of the random effect is larger
      ($\sigma^2_\alpha > \sigma^2_\epsilon$),
      $\operatorname{var}\left(\hat{\beta}_1^\star\right) <
      \operatorname{var}\left(\hat{\beta}_1^\dagger\right)$, so the change model
      is more efficient. Otherwise if $\sigma^2_\alpha < \sigma^2_\epsilon$, the
      follow-up model is more efficient.

      The ANCOVA model is more interesting. From Equation
      \ref{eqn:p1_ancova_estimator_variance}, when $\hat{\gamma} = 0$,
      efficiency is the same as the follow-up model, and when
      $\hat{\gamma} = 1$, efficiency is the same as the change model.
      $\operatorname{var}\left(\hat{\beta}_1^\ddagger\right)$ is a strictly
      convex function of $\hat{\gamma}$ which is minimized at
      \begin{equation}
        \hat{\gamma}^* = \frac{\sigma_\alpha^2}{\sigma_\alpha^2 + \sigma_\epsilon^2}.
        \label{eqn:p1_gamma_hat_optimal}
      \end{equation}
      By the Gauss-Markov theorem that states that the ordinary least squares
      estimate gives the lowest variance estimate for unbiased estimators, we
      must have that $\hat{\gamma} = \hat{\gamma}^*$, so
      \begin{align}
        \operatorname{var}\left(\hat{\beta}_1^\ddagger\right)
        &=
        \frac{2}{n}\left(
          \left(1 - \hat{\gamma}^*\right)^2\sigma_\alpha^2 +
          \left(\hat{\gamma}^*\right)^2\sigma_\epsilon^2 + \sigma_\epsilon^2
          \right) \nonumber \\
        &= \frac{2}{n}\left(
          \frac{\sigma_\alpha^2\sigma_\epsilon^2}{\sigma_\alpha^2 + \sigma_\epsilon^2}
          +
          \sigma_\epsilon^2
          \right) 
        \lneq \frac{2}{n}\left(
          \max\left(\sigma_\alpha^2, \sigma_\epsilon^2\right) + \sigma_\epsilon^2
          \right),
          \label{eqn:p1_ancova_estimator_variance_final}
      \end{align}            
      which results in $\hat{\beta}_1^\ddagger$ being a more efficient estimator
      than both $\hat{\beta}_1^\dagger$ and $\hat{\beta}_1^\star$. The behavior
      of $\hat{\gamma}$ will be investigated more fully in the next two parts.
    \end{description}
  \item Obtain an expression for $\hat{\gamma}$, in as simple a form as you can
    find.
    \begin{description}
    \item[Solution:] From Equation \ref{eqn:p1_ancova_estimator_dist}, we have that
      \begin{align}
        \hat{\gamma}
        &= \left(\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}\left(X^\ddagger\right)^\intercal Y_{:,1}\right)_3 \label{eqn:p1_gamma_hat}\\
        &= \sum_{k=1}^3 \left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)^{-1}_{3k}
          \left(\left(X^\ddagger\right)^\intercal Y_{:,1}\right)_{k} \nonumber\\
        &= 
          -n\frac{\sum_{i=1}^n Y_{i0}}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=1}^{2n}Y_{i1} +
          n\frac{\sum_{i=1}^n Y_{i0} - \sum_{i=n+1}^{2n}Y_{i0}}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=n+1}^{2n}Y_{i1}
          \nonumber\\
        &~~~+ \frac{n^2}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \sum_{i=1}^{2n}Y_{i0}Y_{i1} \nonumber\\
        &= \frac{n}{\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)}
          \left(n\sum_{i=1}^{2n}Y_{i0}Y_{i1}
          -
          \sum_{i=1}^n Y_{i0}\sum_{i=1}^n Y_{i1}
          -
          \sum_{i=n+1}^{2n} Y_{i0}\sum_{i=n+1}^{2n} Y_{i1}
          \right),
          \nonumber
      \end{align}
      where
      $\left. n\middle/\det\left(\left(X^\ddagger\right)^\intercal
          X^\ddagger\right)\right.$ can be obtained from Equation
      \ref{eqn:p1_determinant_expanded}. One can also write Equation
      \ref{eqn:p1_gamma_hat} in terms of empirical variance estimates as in
      Equation \ref{eqn:p1_gamma_hat_variance}.

      \begin{align}
        \frac{1}{n}\det\left(\left(X^\ddagger\right)^\intercal X^\ddagger\right)
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 - \left(
          \sum_{i=1}^{n}Y_{i0}\right)\left(\sum_{i=n+1}^{2n}Y_{i0}\right) \nonumber\\
        &~~~- \left(\sum_{i=n+1}^{2n}Y_{i0}\right)\left(
          2\sum_{i=n + 1}^{2n} Y_{i0} - \sum_{i=1}^{2n}Y_{i0}
          \right) \nonumber\\
        &= n\sum_{i=1}^{2n}Y_{i0}^2 - \left(\sum_{i=1}^{n}Y_{i0}\right)^2 -
          \left(\sum_{i=n+1}^{2n}Y_{i0}\right)^2.
          \label{eqn:p1_determinant_expanded}
      \end{align}

    \end{description}
  \item On the basis of the previous question, or otherwise, give intuitive
    explanations for the efficiency results in Part \ref{part:p1_variance}.
    \begin{description}
    \item[Solution:] Denote the MLE estimates of the covariance between $Y_{i0}$
      and $Y_{i1}$ without and with the intervention by
      \begin{align}
        \hat{\operatorname{cov}}\left(
        Y_{i0}, Y_{i1} \mid x_{i1} = 0
        \right)
        &= \frac{1}{n}\sum_{i=1}^{n} Y_{i0}Y_{i1}
        - \left(\frac{1}{n}\sum_{i=1}^{n} Y_{i0}\right)
          \left(\frac{1}{n}\sum_{i=1}^{n} Y_{i1}\right) \label{eqn:p1_conditional_covariance}\\
        \hat{\operatorname{cov}}\left(
        Y_{i0}, Y_{i1} \mid x_{i1} = 1
        \right)
        &= \frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}Y_{i1}
        - \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}\right)
          \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i1}\right),
          \nonumber
      \end{align}
      respectively. Similarly, we can denote the MLE of the variances of
      $Y_{i0}$ without and with the intervention by
      \begin{align}
        \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 0\right)
        &= \frac{1}{n}\sum_{i=1}^n Y_{i0}^2 - \left(\frac{1}{n}\sum_{i=1}^n Y_{i0}\right)^2
        \label{eqn:p1_conditional_variance}\\
        \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 1\right)
        &= \frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}^2 -
          \left(\frac{1}{n}\sum_{i=n+1}^{2n} Y_{i0}\right)^2,
          \nonumber
      \end{align}
      respectively. Substituting Equations \ref{eqn:p1_conditional_covariance}
      and \ref{eqn:p1_conditional_variance} into the numerator and denominator
      of Equation \ref{eqn:p1_gamma_hat}, respectively, we can write
      \begin{equation}
        \hat{\gamma} = 
        \frac{\hat{\operatorname{cov}}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 0\right) +
          \hat{\operatorname{cov}}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 1\right)}
        {\hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 0\right) +
          \hat{\operatorname{var}}\left(Y_{i0} \mid x_{i1} = 1\right)},
        \label{eqn:p1_gamma_hat_variance}
      \end{equation}
      so we can interpret $\gamma$ as the overall autocorrelation between
      $Y_{i0}$ and $Y_{i1}$. Based on the true model, we can compute
      \begin{align*}
        \operatorname{var}\left(Y_{i0}\right)
        &= \operatorname{var}\left(\epsilon_{i0}\right) + \operatorname{var}\left(\alpha_i\right)
        = \sigma_\epsilon^2 + \sigma_\alpha^2 \\
        \operatorname{cov}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 0\right)
        &= \operatorname{cov}\left(Y_{i0}, Y_{i1} \mid x_{i1} = 1\right) \\
        &= \mathbb{E}\left[\left(\alpha_i + \epsilon_{i0}\right)\left(
          \alpha_i + \epsilon_{i1}
          \right)\right] \\
        &= \operatorname{var}\left(\alpha_i\right) = \sigma^2_\alpha,
      \end{align*}
      so the expected value of $\hat{\gamma}$ is
      \begin{equation}
        \mathbb{E}\left[\hat{\gamma}\right] \approx \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_\epsilon}
        \label{eqn:p1_gamma_hat_expected}     
      \end{equation}
      for large $n$ by Stutsky's theorem, which is the value in Equation
      \ref{eqn:p1_gamma_hat_optimal} that minimizes the variance, so Equation
      \ref{eqn:p1_gamma_hat_variance} agrees with our result in Equation
      \ref{eqn:p1_ancova_estimator_variance_final}.

      Indeed, results from Section 3 of
      \href{https://projecteuclid.org/euclid.aoms/1177706717}{Unbiased
        Estimation of Certain Correlation Coefficients} tell us that
      $\mathbb{E}\left[\hat{\gamma}\right] =
      \frac{\sigma_\alpha^2}{\sigma_\alpha^2 + \sigma_\epsilon^2}$.
    \end{description}
  \item Briefly discuss the implications on inference under each of the three
    models in the situation in which the intervention is non-randomized.
    \begin{description}
    \item[Solution:] Suppose the intervention is non-randomized, that is,
      $\operatorname{cov}\left(Y_{i0}, x_{i1}\right) \neq 0$. This implies that
      at least one of $\operatorname{cov}\left(\alpha_i, x_{i1}\right) \neq 0$
      or $\operatorname{cov}\left(\epsilon_{i0}, x_{i1}\right) \neq 0$ is true.

      The various error terms may no longer be centered
      \begin{align*}
        \epsilon_{i1}^\dagger
        &= \alpha_i + \epsilon_{i1}  \\
        \epsilon_{i1}^\star
        &= -\epsilon_{i0} + \epsilon_{i1} \\
        \epsilon_{i1}^\ddagger
        &= \left(1-\gamma\right)\alpha_i - \gamma\epsilon_{i0} + \epsilon_{i1}
      \end{align*}
      at $0$ depending on the nature of the covariance which violates an
      assumption of the Gauss-Markov theorem, so we may no longer obtain an
      unbiased estimate of $\beta_1$.

      Specifically, we would find
      \begin{align*}
        \mathbb{E}\left[\hat{\beta}_1^\dagger\right]
        &= \beta_1 +
          \left(
          \mathbb{E}\left[\epsilon_{i1}^\dagger \mid x_{i1} = 1\right]
          -
          \mathbb{E}\left[\epsilon_{i1}^\dagger \mid x_{i1} = 0\right]
          \right)
        \\
        \mathbb{E}\left[\hat{\beta}_1^\star \right]
        &= \beta_1 +
          \left(
          \mathbb{E}\left[\epsilon_{i1}^\star \mid x_{i1} = 1\right]
          -
          \mathbb{E}\left[\epsilon_{i1}^\star \mid x_{i1} = 0\right]
          \right) \\
        \mathbb{E}\left[\hat{\beta}_1^\ddagger \right]
        &= \beta_1 +
          \left(
          \mathbb{E}\left[\epsilon_{i1}^\ddagger \mid x_{i1} = 1\right]
          -
          \mathbb{E}\left[\epsilon_{i1}^\ddagger \mid x_{i1} = 0\right]
          \right).
      \end{align*}
      So, if $x_{i1}$ is only correlated with $\epsilon_{i0}$,
      $\hat{\beta}_1^\dagger$ will still be an unbiased estimator. If $x_{i1}$
      is only correlated with $\alpha_{i}$, $\hat{\beta}_1^\star$ will still be
      unbiased. Since $\epsilon_{i1}^\ddagger$ is function of both
      $\epsilon_{i0}$ and $\alpha_i$, $\hat{\beta}_1^\ddagger$ will no longer be
      an unbiased estimator.
    \end{description}
  \end{enumerate}
\item Again in the context of child mortality, let
  \begin{align}
    s_1 &= \mathbb{P}\left(\text{survived first year}\right)
          \nonumber\\
    s_2 &= \mathbb{P}\left(\text{survived years 1--5} \mid \text{survived first year}\right)
          \nonumber\\
    s_3 &= \mathbb{P}\left(\text{survived first 5 years}\right) = s_1 \times s_2.
          \label{eqn:p2_survival_rates}
  \end{align}

  Let $\hat{s}_{1i}$, $\hat{s}_{2i}$, and $\hat{s}_{3i}$ be estimates of
  $s_{1i}$, $s_{2i}$, and $s_{3i}$ for $i = 1,\ldots,n$ areas in a country. Let
  $Y_i = \log \hat{s}_{1i}$, $Z_i = \log \hat{s}_{2i}$, and
  $X_i= \log\hat{s}_{3i}$.

  Let
  \begin{align}
    \mathbb{E}\left[Y_i\right]
    &= \mu_1 \nonumber\\
    \operatorname{var}\left(Y_i\right)
    &= \Sigma_{11} \nonumber\\
    \mathbb{E}\left[Z_i\right]
    &= \mu_2 \nonumber\\
    \operatorname{var}\left(Z_i\right)
    &= \Sigma_{22} \nonumber\\
    \operatorname{cov}\left(Y_i,Z_i\right)
    &= \Sigma_{12}. \label{eqn:p2_parameters}
  \end{align}

  \begin{enumerate}
  \item Suppose $Y_i$ and $X_i$ have a bivariate normal distribution. Write down
    the mean vector and variance-covariance matrix for $Y_i$ and $X_i$.
    \begin{description}
    \item[Solution:] Let $U_i$ and $V_i$ be independent and identically
      distributed standard normal random variables. Then, we must have that
      \begin{equation}
        \begin{pmatrix}
          X_i \\
          Y_i
        \end{pmatrix} = \mu + A\begin{pmatrix}
          U_i \\
          V_i
        \end{pmatrix},
        \label{eqn:p2_bivariate_normal_definition}
      \end{equation}
      where $AA^\intercal = \Sigma$ if $\begin{pmatrix}
        X_i & Y_i
      \end{pmatrix}^\intercal$ is drawn from a bivariate normal. From Equation
      \ref{eqn:p2_parameters}, we can choose
      \begin{equation}
        A = \begin{pmatrix}
          \sqrt{\Sigma_{11}} & 0 \\
          \frac{\Sigma_{12}}{\sqrt{\Sigma_{11}}} &
          \sqrt{\frac{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2}{\Sigma_{11}} }
        \end{pmatrix}.
        \label{eqn:p2_bivariate_normal_A}
      \end{equation}
      
      The parameterization must be
      \begin{equation}
        \begin{pmatrix}
          X_i \\
          Y_i
        \end{pmatrix} \sim
        \operatorname{Normal}\left(
          \begin{pmatrix}
            \mu_1 \\ \mu_2
          \end{pmatrix},
          \begin{pmatrix}
            \Sigma_{11} & \Sigma_{12} \\
            \Sigma_{12} & \Sigma_{22}
          \end{pmatrix}
        \right).
        \label{eqn:p2_xy_dist}
      \end{equation}
    \end{description}
  \item Show that $\mathbb{E}\left[Y_i \mid X_i = x\right]$ takes the form of a
    simple linear regression model and identify $\beta_0$ and
    $\beta_1$. Carefully interpret $\beta_1$.

    \begin{description}
    \item[Solution:] From Equation \ref{eqn:p2_bivariate_normal_definition}, we
      have that
      \begin{align*}
        X_i &= \mu_{1} + A_{11}U_i + A_{12}V_i \\
        Y_i &= \mu_{2} + A_{21}U_i + A_{22}V_i.
      \end{align*}
      If we fix $X_i = x$ and use the defintion of $A$ in Equation
      \ref{eqn:p2_bivariate_normal_A}, we have that
      \begin{align}
        Y_i
        &= \mu_2 + A_{21}U + \frac{A_{22}}{A_{12}}\left(
          x - A_{11}U - \mu_1
          \right) \nonumber\\
        &= \mu_2 + \frac{A_{21}}{A_{11}}U + A_{22}V \nonumber\\
        &= \mu_2 + \frac{\Sigma_{12}}{\Sigma_{22}}\left(x - \mu_1\right)
          + \sqrt{\frac{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2}{\Sigma_{11}}}V \nonumber\\
        &\sim \mathcal{N}\left(
          \mu_2 + \frac{\Sigma_{12}}{\Sigma_{11}}\left(x - \mu_1\right)
          ,
          \frac{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2}{\Sigma_{11}}
          \right). \label{eqn:p2_conditional_y_dist}
      \end{align}

      Equation \ref{eqn:p2_conditional_y_dist} gives us
      \begin{equation}
        \mathbb{E}\left[Y_i \mid X_i = x\right] =
        \mu_2 + \frac{\Sigma_{12}}{\Sigma_{11}}\left(x - \mu_1\right)
        = \mu_2 - \frac{\Sigma_{12}}{\Sigma_{11}}\mu_1 + \frac{\Sigma_{12}}{\Sigma_{11}}x.
        \label{eqn:p2_conditional_y_expectation}
      \end{equation}

      Imagine fitting the model
      \begin{equation}
        Y_{i} = \beta_0 + \beta_1X_i + \epsilon_i.
        \label{eqn:p2_y_model}
      \end{equation}

      Equations \ref{eqn:p2_conditional_y_dist} and
      \ref{eqn:p2_conditional_y_expectation} show that the ordinary least
      squares estimator for Equation \ref{eqn:p2_y_model} is
      \begin{equation}
        \begin{pmatrix}
          \hat{\beta}_0 \\
          \hat{\beta}_1
        \end{pmatrix}
        \mid X
        \sim \mathcal{N}\left(
        \begin{pmatrix}
          \mu_2 - \frac{\Sigma_{12}}{\Sigma_{11}}\mu_1 \\
          \frac{\Sigma_{12}}{\Sigma_{11}}
        \end{pmatrix},
        \frac{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2}{\Sigma_{11}}
        \left(X^\intercal X\right)^{-1}
      \right),
      \label{eqn:p2_estimator_dist}
    \end{equation}
    where $X$ is a $n \times 2$ matrix with $1$s in the first column and
    $X_{i2} = X_i$ in the second column.

    From Equations \ref{eqn:p2_y_model} and \ref{eqn:p2_estimator_dist}, we see
    that $\beta_1$ quantifies the correlation between $X_i$ and $Y_i$: given an
    observation $X_i$ that is $1$ unit greater, we would expect to observe a
    difference of $\beta_1$ units in $Y_i$. Note that
    \begin{equation}
      \mathbb{E}\left[\hat{\beta}_1\right] = \frac{\Sigma_{12}}{\Sigma_{11}}
      = \rho\sqrt{\frac{\Sigma_{22}}{\Sigma_{11}}},      
    \end{equation}
    where $\rho = \frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}$ is the
    Pearson correlation coefficient, so $\beta_1$ is the correlation between
    $X_i$ and $Y_i$ scaled by the standard errors.

    Rewriting the model in Equation \ref{eqn:p2_y_model} in terms of $s_{1i}$
    and $s_{2i}$, we have that
    \begin{align*}
      \log s_{1i}
      &= \beta_0 + \beta_1\log s_{3i} + \epsilon_i \\
      s_{1i}
      &= \exp\left(\beta_0\right) \times s_{3i}^{\beta_1} \times \exp\left(\epsilon_i\right) \\
      s_{1i}^{1/\beta_1} =
      \left(\frac{1}{s_{1i}}\right)^{-1/\beta_1}
      &= \exp\left(\frac{\beta_0}{\beta_1}\right) \times s_{3i} \times
        \exp\left(\frac{\epsilon_i}{\beta_1}\right).
    \end{align*}
    $\beta_1$ indicates how much to weight the probability of having survived
    the first 5 years in computing the expected probability of having survived
    the first year. If one interprets the probability of dying as exponential
    process, $1/\beta_1$ can be seen as how fast the probability of survival
    decays over a 5 year period. Specifically, with each year the probability of
    suriviving decays $\frac{1}{5\beta_1}\log s_{1i}$ on average.

    Another way to interpret this is by writing Equation \ref{eqn:p2_y_model} as
    \begin{equation*}
      s_{1i}
      = \exp\left(\beta_0\right) \times s_{3i}^{\beta_1 - 1} \times s_{3i}
      \times \exp\left(\epsilon_i\right).
    \end{equation*}
    Then, $\exp\left(\beta_0\right) \times s_{3i}^{\beta_1 - 1}$ becomes an
    estimate of $s_{2i}^{-1}$ since $s_{3i} = s_{1i} \times s_{2i}$.
    
    Note that $s_{3i} \leq 1$, so $X_i \leq 0$. So large values of $\beta_1$
    drive $s_{i1}$ to $0$. $0$ would mean that $X_i$ has no association with
    $Y_i$.
  \end{description}
\item The data on the website contain estimates of the log first year survival
  ($Y_i$) and log five year survival ($X_i$) for $n = 47$ areas in Kenya in the
  period 2000--2004. Fit a linear model and hence summarize the association
  between first year and first five year survival, in these data.

  \begin{table}
    \centering
    \input{p2_estimates.tex}
    \caption{Least squares estimates for the model in Equation \ref{eqn:p2_y_model}.}
    \label{tab:p2_y_model_estimates}
  \end{table}
  
  \begin{description}
  \item[Solution:] The result of fitting a linear model can be found in Table
    \ref{tab:p2_y_model_estimates}. The association of $Y_{i}$ with $X_{i}$ is
    statistically significant.

    Over $5$ years, the probability of survival decayed at a rate of
    $\frac{1}{5\beta_1}\log s_{1i} \approx 0.291\log s_{1i}$ each year on
    average.
  \end{description}
\item Now suppose we are presented with a new area, for which only five year
  survival is available and known to be 0.95. Obtain point estimates and a
  95\% confidence interval for:
  \begin{enumerate}
  \item Surviving the first year.
  \item Death within the first year.
  \item Death between ages 1 and 5, given survival until age 1.
  \item Death between ages 1 and 5.
  \end{enumerate}

  \begin{description}
  \item[Solution:] Note that $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$,
    so a prediction at a given point $x_0$ is
    \begin{equation}
      \hat{y}_0
      = \left(\bar{Y} - \hat{\beta}_1\bar{X}\right) + \hat{\beta}_1x_0 + \epsilon_0
      = \bar{Y} + \hat{\beta}_1\left(x_0 - \bar{X}\right)  + \epsilon_0.
    \end{equation}    
  \end{description}
\item Can you see any problems with this model?
  \begin{description}
  \item[Solution:] Yes. Recall from the problem description that
    $\exp\left(X_i\right) = s_{3i} = s_{1i} \times s_{2i} =
    \exp\left(Y_i\right)\exp\left(Z_i\right) = \exp\left(Y_i + Z_i\right)$.

    Let $\sigma^2 = \frac{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2}{\Sigma_{11}}.$
    We can estimate it with the MLE:
    \begin{equation}
      \hat{\sigma}^2 = \frac{\hat{\Sigma}_{11}\hat{\Sigma}_{22} - \hat{\Sigma}_{12}^2}
      {\hat{\Sigma}_{11}},
    \end{equation}
    where $\hat{\Sigma}$ is the sample covariance matrix for the $X_i$ and $Y_i$.

    For the variance, we have that
    \begin{align*}
      \operatorname{var}\left(
      \hat{y}_0
      \mid x_0, X
      \right)
      &= \sigma^2 + \frac{\sigma^2}{n} + \sigma^2\left(X^\intercal X\right)^{-1}_{22}
        \left(x - \bar{X}\right)^2.
    \end{align*}
    
    Since the sample size is fairly large, using the plug-in estimators, we have
    an approximate normal distribution,
    \begin{equation}
      \hat{y} \mid x_0, X \sim \mathcal{N}\left(
        \hat{\beta}_0 + \hat{\beta}_1x_0,
        \hat{\sigma}^2\left(
          1 + \frac{1}{n} + \left(X^\intercal X\right)^{-1}_{22}\left(x - \bar{X}\right)^2
        \right)
      \right).
    \end{equation}    
  \end{description}
\end{enumerate}
\end{enumerate}
\end{document}
