{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my attempt to recreate the model from [chredlin_model_truth.ipynb](chredlin_model_truth.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares\n",
    "\n",
    "Suppose our model is that $Y \\sim \\mathcal{N}\\left(X^\\intercal \\beta, \\sigma^2\\right)$.\n",
    "\n",
    "## Parameter Estimates\n",
    "\n",
    "We want to find $\\beta$ such that $L\\left(\\beta\\right) = \\left\\lVert X\\beta - y \\right\\rVert_2^2$ is minimized. That is, we want to find the projection of $y$ onto to the hyperplane spanned by the columns of $X$. Thus, we must have that $X^\\intercal\\left(y - X\\hat{\\beta}\\right) = 0$ since the residuals will orthogonal to the columns of $X$ if $X\\hat{\\beta}$ is the projection that minimizes the squared error. Solving for $\\hat{\\beta}$, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\left(X^\\intercal X\\right)^{-1} X^\\intercal y.\n",
    "\\end{equation}\n",
    "\n",
    "## Residual Standard Error\n",
    "\n",
    "Let us derive an unbiased estimator for residual standard error. Consider the residual random vector.\n",
    "\n",
    "\\begin{equation}\n",
    "R = y - X\\hat{\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "As stated earlier, the residuals are orthogonal to hyperplane spanned by the columns of $X$, so they must lie in some orthonormal hyperplane of $N - p$ vectors, where $p = \\dim(\\beta)$. Thus, the residuals are $y$ projected down to this space.\n",
    "\n",
    "Let $w_1,\\ldots,w_{n-p}$ be an orthonormal basis of this space. Let $W$ be matrix with these basis vectors as the columns.\n",
    "\n",
    "We have that\n",
    "\n",
    "\\begin{align}\n",
    "R &= y - X\\hat{\\beta} \\\\\n",
    "&= W\\left(W^\\intercal y\\right) \\\\\n",
    "&= W\\left(W^\\intercal\\left(X\\beta + \\sigma\\epsilon\\right)\\right) \\\\\n",
    "&= W\\left(W^\\intercal X\\right)\\beta + \\sigma W\\left(W^\\intercal\\epsilon\\right) \\\\\n",
    "&= \\sigma W\\left(W^\\intercal\\epsilon\\right).\n",
    "\\end{align}\n",
    "\n",
    "Now, $W^\\intercal\\epsilon \\sim \\mathcal{N}\\left(0, I_{n-p}\\right)$. To see this, note that the $i$th entry is $\\sum_{j=1}^n w_{ij}\\epsilon_j \\sim \\mathcal{N}\\left(0, 1\\right)$, and for $i \\neq i^\\prime$,\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{Cov}\\left(\\left(W^\\intercal\\epsilon\\right)_i, \\left(W^\\intercal\\epsilon\\right)_{i^\\prime}\\right) &=\n",
    "\\mathbb{E}\\left[\n",
    "\\left(\\sum_{j=1}w_{ij}\\epsilon_j\\right)\\left(\\sum_{k=1}w_{i^\\prime k}\\epsilon_k\\right)\n",
    "\\right] \\\\\n",
    "&= \\sum_{j=1}^n\\mathbb{E}\\left[w_{ij}w_{i^\\prime j} \\epsilon_j^2\\right] +\n",
    "2\\sum_{j=1}^{n-1}\\sum_{k=j+1}^n \\mathbb{E}\\left[w_{ij}w_{i^\\prime k} \\epsilon_j\\epsilon_k\\right] \\\\\n",
    "&= w_i^\\intercal w_{i^\\prime} + 2\\sum_{j=1}^{n-1}\\sum_{k=j+1}^n w_{ij}w_{i^\\prime k} \\mathbb{E}\\left[\\epsilon_j\\epsilon_k\\right] \\\\\n",
    "&= 0,\n",
    "\\end{align}\n",
    "where the first term disappears by since the two vectors are orthonormal, and the second term disappears because of independence of the errors.\n",
    "\n",
    "Thus, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "R^\\intercal R = \\sigma^2 \\left(W^\\intercal\\epsilon\\right)^\\intercal W^\\intercal W \\left(W^\\intercal\\epsilon\\right) =\\sigma^2 \\left(W^\\intercal\\epsilon\\right)^\\intercal\\left(W^\\intercal\\epsilon\\right)\n",
    "\\sim \\sigma^2 \\chi^2_{n-p}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}\\left[R^\\intercal R\\right] = \\sigma^2\\left(n - p\\right)\n",
    "\\Rightarrow\n",
    "\\mathbb{E}\\left[\\frac{\\sum_{i=1}^n \\left(y - X\\hat{\\beta}\\right)^2}{n-p}\\right] = \\sigma^2.\n",
    "\\end{equation}\n",
    "\n",
    "Our consistent estimator is\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n \\left(y - X\\hat{\\beta}\\right)^2}{n-p}.}\n",
    "\\end{equation}\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "We can rewrite $y$ as $y = X\\beta + \\sigma \\epsilon$, where each element of $\\epsilon$ is drawn from $\\mathcal{N}\\left(0, 1\\right)$. Substituting, we have that\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} &= \\left(X^\\intercal X\\right)^{-1}X^\\intercal\\left(X\\beta + \\sigma\\epsilon\\right) \\\\\n",
    "&= \\beta + \\sigma\\left(X^\\intercal X\\right)^{-1}X^\\intercal \\epsilon.\n",
    "\\end{align}\n",
    "\n",
    "Thus, $\\hat{\\beta}_j \\sim \\mathcal{N}\\left(\\beta_j, \\sigma^2\\left(X^\\intercal X\\right)^{-1}_{jj}\\right)$.\n",
    "\n",
    "This gives us that\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\left(X^\\intercal X\\right)^{-1}_{jj}}} \\sim\n",
    "\\mathcal{N}\\left(0, 1\\right).\n",
    "\\end{equation}\n",
    "\n",
    "From the previous part,\n",
    "\n",
    "\\begin{equation}\n",
    "(n - p)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{n-p}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{\\beta}$ and $\\hat{\\sigma}^2$ are independent by Basu's theorem: $\\hat{\\sigma}^2$ is an ancillary statistic that does not depend on the model parameters, $\\beta$. Thus, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\left.\n",
    "\\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\left(X^\\intercal X\\right)^{-1}_{jj}}}\n",
    "\\middle/\n",
    "\\sqrt{\\frac{(n - p)\\frac{\\hat{\\sigma}^2}{\\sigma^2}}{n-p}}\n",
    "\\right. \n",
    "= \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2\\left(X^\\intercal X\\right)^{-1}_{jj}}}\n",
    "\\sim t_{n-p}.\n",
    "\\end{equation}\n",
    "\n",
    "That is, we have $t$ distribution with $n - p$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Model Parameters\n",
    "\n",
    "The code for `LinearRegression` can be found in [linear_regression.py](https://github.com/ppham27/stat570/tree/master/stat570/linear_model/linear_regression.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual standard error is 0.3345267301243203.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>std_error</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(intercept)</th>\n",
       "      <td>-1.185540</td>\n",
       "      <td>1.100255</td>\n",
       "      <td>-1.077514</td>\n",
       "      <td>0.287550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.009502</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>3.816831</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.008766</td>\n",
       "      <td>4.546588</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theft</th>\n",
       "      <td>-0.010295</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>-3.653264</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>3.037749</td>\n",
       "      <td>0.004134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_income</th>\n",
       "      <td>0.345762</td>\n",
       "      <td>0.400123</td>\n",
       "      <td>0.864137</td>\n",
       "      <td>0.392540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             estimate  std_error  t-statistic   p-value\n",
       "(intercept) -1.185540   1.100255    -1.077514  0.287550\n",
       "race         0.009502   0.002490     3.816831  0.000449\n",
       "fire         0.039856   0.008766     4.546588  0.000048\n",
       "theft       -0.010295   0.002818    -3.653264  0.000728\n",
       "age          0.008336   0.002744     3.037749  0.004134\n",
       "log_income   0.345762   0.400123     0.864137  0.392540"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chredlin import load_data\n",
    "from stat570.linear_model.linear_regression import LinearRegression\n",
    "\n",
    "chredlin = load_data()\n",
    "\n",
    "COVARIATES = ['race', 'fire', 'theft', 'age', 'log_income']\n",
    "RESPONSE = 'involact'\n",
    "\n",
    "linear_model = LinearRegression.from_data_frame(chredlin, COVARIATES, RESPONSE)\n",
    "\n",
    "with open('p1_residual_standard_error.txt', 'w') as f:\n",
    "    f.write(str(np.sqrt(linear_model.residual_variance_)))\n",
    "\n",
    "with open('p1_model_parameters.tex', 'w') as f:\n",
    "    f.write(linear_model.coefficients_.to_latex())\n",
    "\n",
    "print('Residual standard error is {}.'.format(np.sqrt(linear_model.residual_variance_)))\n",
    "linear_model.coefficients_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Confidence Interval\n",
    "\n",
    "Let us $0$-index the columns of $X$ so that the $0$th column is all $1$s. Let us $0$-index $\\beta$ so the $0$th entry is the intercept. Since $\\hat{\\beta}$ satisfies $\\left(X^\\intercal X\\right)\\hat{\\beta} = X^\\intercal y$, we have that the intercept estimate is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\sum_{j=1}^p \\hat{\\beta}_j \\bar{X}_{:,j}.\n",
    "\\end{equation}\n",
    "\n",
    "Consider trying to predict $\\hat{y} = x^\\intercal\\hat{\\beta}$ for some $x$. We have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\bar{y} + \\sum_{j=1}^p \\left(x_i - \\bar{X}_{:,j}\\right)\\hat{\\beta}_j,\n",
    "\\end{equation}\n",
    "\n",
    "so the variance of the prediction increases with values far from data.\n",
    "\n",
    "Let $\\bar{X}$ be the vector of column-wise means of $X$. Since $\\bar{\\epsilon}$ is an ancillary statistic, this can also be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} \\mid x \\sim \\mathcal{N}\\left(\n",
    "x^\\intercal \\beta, \\sigma^2 \\left(\\frac{1}{n} + \\left(x - \\bar{X}\\right)^\\intercal\\left(X^\\intercal X\\right)^{-1}\\left(x - \\bar{X}\\right)\\right)\n",
    "\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Following the same method as before, if we replace $\\beta$ with $\\hat{\\beta}$ and $\\sigma^2$ with $\\hat{\\sigma}^2$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\hat{y} - x^\\intercal\\hat{\\beta}}{\n",
    "\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\left(x - \\bar{X}\\right)^\\intercal\\left(X^\\intercal X\\right)^{-1}\\left(x - \\bar{X}\\right)\\right)}\n",
    "}\n",
    "\\sim t_{n-p}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Model\n",
    "\n",
    "`log_income` doesn't add much value to the model. From [chredlin_explore.ipynb](chredlin_explore.ipynb), we see this is because income correlates so strongly with `race` and `fire`. Let's remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual standard error is 0.3335165792871865.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>std_error</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(intercept)</th>\n",
       "      <td>-0.243118</td>\n",
       "      <td>0.145054</td>\n",
       "      <td>-1.676054</td>\n",
       "      <td>0.101158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>4.296913</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.036646</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>4.629173</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theft</th>\n",
       "      <td>-0.009592</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>-3.565847</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>2.994369</td>\n",
       "      <td>0.004595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             estimate  std_error  t-statistic   p-value\n",
       "(intercept) -0.243118   0.145054    -1.676054  0.101158\n",
       "race         0.008104   0.001886     4.296913  0.000100\n",
       "fire         0.036646   0.007916     4.629173  0.000035\n",
       "theft       -0.009592   0.002690    -3.565847  0.000921\n",
       "age          0.007210   0.002408     2.994369  0.004595"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model_custom = LinearRegression.from_data_frame(\n",
    "    chredlin,\n",
    "    [covariate for covariate in COVARIATES if covariate != 'log_income'],\n",
    "    RESPONSE)\n",
    "\n",
    "with open('p1_residual_standard_error_custom.txt', 'w') as f:\n",
    "    f.write(str(np.sqrt(linear_model_custom.residual_variance_)))\n",
    "\n",
    "with open('p1_model_parameters_custom.tex', 'w') as f:\n",
    "    f.write(linear_model_custom.coefficients_.to_latex())\n",
    "\n",
    "print('Residual standard error is {}.'.format(np.sqrt(linear_model_custom.residual_variance_)))\n",
    "linear_model_custom.coefficients_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
