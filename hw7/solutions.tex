\documentclass[letterpaper,11pt]{article}

\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{pdflscape}
\usepackage{subcaption}

\title{Coursework 7: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle
\begin{enumerate}
\item Create a binary variable $Z_i$, with $Z_i = 0$ corresponding to
  $Y_i \in \{0,1\}$ and $Z_i = 1$ corresponding to $Y_i \in \{2,3\}$. Let
  $q\left(x_i\right) = \mathbb{P}(Z_i = 1 \mid x_i)$, with
  $\mathbf{x}_i = \begin{pmatrix} 1 & x_{1i} & x_{2i}
  \end{pmatrix}^\intercal$, represent the probability of mental impairment being
  \emph{Moderate} or \emph{Impaired}, given covariates $\mathbf{x}_i$,
  $i = 1,\ldots,n = 40$.  Provide a single plot that shows the association
  between $q\left(x_i\right)$ and $x_{1i}$ and $x_{2i}$, on a response scale
  you feel is appropriate. Comment on the plot.

  \begin{figure}[h]
    \centering
    \includegraphics{p1_descriptive.pdf}
    \caption{Orange denotes $Z_i = 1$ and blue denotes $Z_i = 0$.}
    \label{fig:p1_descriptive}
  \end{figure}
  
  \begin{description}
  \item[Solution:] See Figure \ref{fig:p1_descriptive}. Conditioned on SES,
    those that are impaired ($Z_i = 1$) have a greater number of life events on
    average.
  \end{description}
\item Suppose $Z_i \mid q_i \sim \operatorname{Binomial}\left(1, q_i\right)$
  independently for $i = 1,\ldots, n = 40$, where $q_i =
  q\left(x_i\right)$. Consider the logistic regression model,
  \begin{equation}
    q\left(x_i\right) = \log\left(
      \frac{q\left(\mathbf{x}_i\right)}{1 - q\left(\mathbf{x}_i\right)}
    \right)
    =
    \mathbf{x}_i^\intercal \bm{\gamma}
    = \gamma_0 + \gamma_1 x_{1i} + \gamma_2 x_{2i},
    \label{eqn:p2_model}
  \end{equation}  
  where $\bm{\gamma} = \begin{pmatrix}\gamma_0 & \gamma_1 & \gamma_2
  \end{pmatrix}^\intercal$. Write down the log-likelihood
  $l\left(\bm{\gamma}\right)$ for the sample $z_i$, $i = 1,\ldots, n$.

  \begin{description}
  \item[Solution:] Solving for $q\left(\mathbf{x}_i\right)$ in Equation
    \ref{eqn:p2_model}, we find
    \begin{equation}
      q\left(\mathbf{x}_i\right)
      = \frac{\exp\left(\mathbf{x}_i^\intercal \bm{\gamma}\right)}
      {1 + \exp\left(\mathbf{x}_i^\intercal \bm{\gamma}\right)}
      = \frac{1}
      {1 + \exp\left(-\mathbf{x}_i^\intercal \bm{\gamma}\right)}.
      \label{eqn:p2_qi}
    \end{equation}

    The likelihood function is
    $L\left( \bm{\gamma} \right) = \prod_{i=1}^n
    \left(q\left(\mathbf{x}_i\right)\right)^{z_i} \left(1 -
      q\left(\mathbf{x}_i\right)\right)^{1 - z_i}$, so the log-likelihood
    function becomes
    \begin{align}
      l\left(\bm{\gamma}\right)
      &= \log L\left( \bm{\gamma} \right)
        = \sum_{i=1}^n \left(
        z_i \log q\left(\mathbf{x}_i\right)
        +
        \left(1 - z_i \right) \log\left(1 - q\left(\mathbf{x}_i\right)\right)
        \right)
        \label{eqn:p2_log_likelihood}\\
      &= \sum_{i=1}^n\left(
        z_i\log\frac{q\left(\mathbf{x}_i\right)}{1 -q\left(\mathbf{x}_i\right)}
        + \log\left(1 -q\left(\mathbf{x}_i\right)\right)
        \right) \nonumber\\
      &= \sum_{i=1}^n
        \left(
        z_i\mathbf{x}_i^\intercal\bm{\gamma} +
        \log\frac{1}{1 + \exp\left(\mathbf{x}_i^\intercal\bm{\gamma}\right)}
        \right)
        = \sum_{i=1}^n
        -\log\left(
        1 + \exp\left(\left(1 - 2z_i\right)\mathbf{x}_i^\intercal\bm{\gamma}\right)
        \right). \nonumber
    \end{align}
  \end{description}
\item Fit the model described in the previous part, and give confidence
  intervals for the odds ratios.

  Carefully interpret these odds ratios.

  \begin{table}[!h]
    \centering
    \input{p3_gamma_hat.tex}
    \caption{Estimates and confidence intervals for $\hat{\bm\gamma}$ using
      maximum likelihood estimation.}
    \label{tab:p3_gamma_hat}
  \end{table}
  
  \begin{description}
  \item[Solution:] Taking the derivative of Equation
    \ref{eqn:p2_log_likelihood}, we have the score function:

    \begin{align}
      S\left(\bm\gamma\right)
      = \nabla^\intercal l\left(\bm\gamma\right)
      &= \sum_{i=1}^n \frac{2z_i - 1}{1 + \exp\left(
        \left(1 - 2z_i\right)\mathbf{x}_i^\intercal\bm{\gamma}
        \right)}\exp\left(
        \left(1 - 2z_i\right)\mathbf{x}_i^\intercal\bm{\gamma}
        \right)\mathbf{x}_i.
        \nonumber\\
      &= \sum_{i=1}^n \frac{2z_i - 1}{1 + \exp\left(
        \left(2z_i - 1\right)\mathbf{x}_i^\intercal\bm{\gamma}
        \right)}\mathbf{x}_i \nonumber\\
      &= X^\intercal\left(
        \mathbf{z} - \mathbf{q}\left(X\right)
        \right),
    \label{eqn:p3_score_function}
    \end{align}
    where
    $\mathbf{z} = \begin{pmatrix}z_1 & z_2 & \cdots & z_n\end{pmatrix}^\intercal$ and
    $\mathbf{q}\left(X\right) = \begin{pmatrix}q_1 & q_2 &
      \cdots & q_n\end{pmatrix}^\intercal$.
    
    From Equation \ref{eqn:p3_score_function}, we have the Fisher information
    matrix:
    \begin{align}
      I_n\left(\bm\gamma\right)
      &= \operatorname{var}\left(S\left(\bm\gamma\right) \mid \bm\gamma\right)
        = \mathbb{E}\left[
        S\left(\bm\gamma\right)
        S\left(\bm\gamma\right)^\intercal
        \mid \bm\gamma
        \right] \nonumber\\
      &= \mathbb{E}\left[
        X^\intercal\left(
        \mathbf{z} - \mathbf{q}\left(X\right)
        \right)
        \left(
        \mathbf{z} - \mathbf{q}\left(X\right)
        \right)^\intercal
        X \mid \bm\gamma
        \right] \nonumber \\
      &= X^\intercal\mathbb{E}\left[
        \left(
        \mathbf{z} - \mathbf{q}\left(X\right)
        \right)
        \left(
        \mathbf{z} - \mathbf{q}\left(X\right)
        \right)^\intercal
        \mid \bm\gamma
        \right]X \nonumber\\
      &= \sum_{i=1}^n 
        q\left(\mathbf{x}_i\right)
        \left(1 - q\left(\mathbf{x}_i\right)\right)
        \mathbf{x}_i\mathbf{x}_i^\intercal
        =
        \sum_{i=1}^n
        \frac{1}{2 + \exp\left(
        -\mathbf{x}_i^\intercal \bm\gamma
        \right) + \exp\left(
        \mathbf{x}_i^\intercal \bm\gamma
        \right)}
        \mathbf{x}_i\mathbf{x}_i^\intercal
        ,
        \label{eqn:p3_fisher_information}
    \end{align}
    where we have used independence of the observations and variance of the
    binomial distribution to get the last line.

    We solve Equation \ref{eqn:p3_score_function},
    $S\left(\hat{\bm{\gamma}}\right) = \mathbf{0}$, to get an estimate for
    $\bm\gamma$. Using Equation \ref{eqn:p3_fisher_information}, we have that
    \begin{equation}
      \hat{\bm\gamma}
      \xrightarrow{\mathcal{D}}
      \mathcal{N}\left(
        \gamma,
        I_n^{-1}\left(\hat{\bm\gamma}\right)
      \right),
      \label{eqn:p3_gamma_hat_dist}
    \end{equation}
    that is, $\hat{\bm\gamma}$ is asymptotically normal.

    Using Equation \ref{eqn:p3_gamma_hat_dist}, we obtain the estimates and
    intervals in Table \ref{tab:p3_gamma_hat}.

    The predicted log odds ratio given some $\mathbf{x}_i$ is
    \begin{equation}
      \hat\theta_i = \mathbf{x}_i^\intercal\hat{\bm\gamma},
      \label{eqn:p3_prediction}
    \end{equation}
    which will have variance
    \begin{equation}
      \operatorname{var}\left(\hat\theta_i\right)
      = \mathbf{x}_i^\intercal\operatorname{var}\left(\hat{\bm\gamma}\right)
      \mathbf{x}_i
      \approx \mathbf{x}_i^\intercal I_n^{-1}\left(\hat{\bm\gamma}\right)
      \mathbf{x}_i,
      \label{eqn:p3_prediction_variance}
    \end{equation}
    using Equation \ref{eqn:p3_gamma_hat_dist}.

    From Equation \ref{eqn:p3_prediction_variance}, we can compute confidence
    intervals for the log odds ratio and exponentiate to get confidence
    intervals for the odds ratio since $\log$ is a monotonic
    transformation. Doing so results in the estimates in Table
    \ref{tab:p3_odds_ratios}.

    The odds ratio is how much more likely one is to have \texttt{Moderate} or
    \texttt{Impaired} mental impairment. Exponentiating Equation
    \ref{eqn:p3_prediction}, we have
    \begin{equation}
      \exp\left(\theta_i\right) =
      \exp\left(\gamma_0\right)
      \exp\left(\gamma_1x_{1i}\right)
      \exp\left(\gamma_2x_{2i}\right).
    \end{equation}
    
    $\exp\left(\gamma_0\right)$ is the expected odds ratio for a subject with
    $0$ SES and no life events. $\exp\left(\gamma_1\right)$ is the expected odds
    ratio between a subject with SES 1 and SES 0. $\exp\left(\gamma_2\right)$ is
    the expected odds ratio for a subject with an additional life event.
  \end{description}
  \begin{table}
    \scriptsize
    \centering
    \input{p3_odds_ratios.tex}
    \caption{Estimates for the odds ratios given $\mathbf{x}_i$ with
      $\hat{\bm\gamma}$.}
    \label{tab:p3_odds_ratios}
  \end{table}
  
\item We will now consider analyses that do not coarsen the data. We begin by
  defining notation in a generic situation. Suppose the random variable, $Y_i$,
  for individual $i$, $i = 1,\ldots,n$, can take values $0,1,2,\ldots,J-1$ (so
  that that there are $J$ levels). Assume that for individual $i$, the data
  follow a multinomial distribution,
  $Y_i \mid p_i \sim \operatorname{Multinomial}\left(1,\mathbf{p}_i\right)$
  independently, where $p_i = \begin{pmatrix}p_{i0} & \cdots & p_{i,Jâˆ’1}
  \end{pmatrix}^\intercal$, and $p_{ij}$ represents the probability
  \begin{equation}
    p_{ij} = \mathbb{P}\left(Y_i = j \mid \mathbf{x}_i\right),
    ~\text{for}~j=0,1,\ldots,J-1,
    \label{eqn:p4_pij}
  \end{equation}
  where $\mathbf{x}_i = \begin{pmatrix} 1 & x_{1i} & x_{2i} & \cdots & x_{ki}
  \end{pmatrix}^\intercal$ for $i = 1,\ldots,n$.

  Suppose the response categories re nominal, that is, have no ordering. In this
  case, we may consider the \emph{generalized logit model}:
  \begin{equation}
    p_{ij} = \frac{\exp\left(\mathbf{x}_i^\intercal\bm\beta_j\right)}{
      \sum_{l=0}^{J-1}\exp\left(\mathbf{x}_i^\intercal\bm\beta_l\right)
    },~\text{for}~j=0,\ldots,J-1,
  \end{equation}
  where
  $\bm\beta_j = \begin{pmatrix} \beta_{j0} & \beta_{j1} & \cdots & \beta_{jk}
  \end{pmatrix}^\intercal$.

  Identifiability may be enforced by taking $\bm\beta_{J-1} = \mathbf{0}$, to
  give
  \begin{equation}
    \log\frac{p_{ij}}{p_{i,J-1}} = \mathbf{x}_i^\intercal\bm\beta_j,
    ~\text{for}~j=0,\ldots,J-2,
    \label{eqn:p4_log_odds_ratio}
  \end{equation}
  with $p_{i,J-1} = 1 - \sum_{j=0}^{J-2} p_{ij}$. Consider the case of $j = 3$
  levels and a single binary covariate $x$ so that that
  $\mathbf{x}_i = \begin{pmatrix} 1 & x_i \end{pmatrix}^\intercal$. Give a
  $3 \times 2$ table containing the probabilities of
  $\mathbb{P}\left(Y = j \mid x\right)$ in terms the $\beta_{jx}$s for rows
  $j = 0,1,2$ and columns $x=0,1$. Hence, give interpretations of
  $\exp\left(\beta_{jx}\right)$ for $j = 0,1,2$ and $x = 0,1$.

  Is the generalized logit model suitable for ordinal data?

  \begin{table}[h]
    \small
    \centering
    \begin{tabular}{lcc}    
      \toprule
      $j$ & $x = 0$ & $x = 1$ \\
      \midrule
      $0$ & $\displaystyle\frac{\exp\left(\beta_{00}\right)}{1 + \exp\left(\beta_{00}\right) + \exp\left(\beta_{10}\right)}$ & $\displaystyle\frac{\exp\left(\beta_{00} + \beta_{01}\right)}{1 + \exp\left(\beta_{00} + \beta_{01}\right) + \exp\left(\beta_{10} + \beta_{11}\right)}$ \\
      $1$ & $\displaystyle\frac{\exp\left(\beta_{10}\right)}{1 + \exp\left(\beta_{00}\right) + \exp\left(\beta_{10}\right)}$ & $\displaystyle\frac{\exp\left(\beta_{10} + \beta_{11}\right)}{1 + \exp\left(\beta_{00} + \beta_{01}\right) + \exp\left(\beta_{10} + \beta_{11}\right)}$ \\
      $2$ & $\displaystyle\frac{1}{1 + \exp\left(\beta_{00}\right) + \exp\left(\beta_{10}\right)}$ & $\displaystyle\frac{1}{1 + \exp\left(\beta_{00} + \beta_{01}\right) + \exp\left(\beta_{10} + \beta_{11}\right)}$ \\
      \bottomrule
    \end{tabular}
    \caption{Multinomial probabilities for various $j$ and $x$.}
    \label{tab:p4_multinomial_probability}
  \end{table}

  \begin{description}
  \item[Solution:] See Table \ref{tab:p4_multinomial_probability} for the table
    of probabilities.

    Equation \ref{eqn:p4_log_odds_ratio} provides a way to interpret the
    $\beta_{jx}$. Let $p_{0j}$ and $p_{1j}$ denote the probabilities when
    $x = 0$ and $x = 1$, respectively. In this case, we have the odds ratios:
    \begin{align*}
      \frac{p_{0j}}{p_{02}} &= \exp\left(\beta_{j0}\right) \\
      \frac{p_{1j}}{p_{12}} &= \exp\left(\beta_{j0}\right)\exp\left(\beta_{j1}\right).
    \end{align*}

    Thus, the coefficients $\beta_{j0}$ are the expected log odds ratio for
    level $j$ relative to level $J - 1 = 2$ when the $x = 0$. $\beta_{j1}$ is
    the expected increase in this log odds ratio when $x = 1$. In this sense, we
    can consider the level $J - 1$ the default case, and
    $\exp\left(\beta_{jx}\right)$ express how much more likely we are to observe
    level $j$.

    This model isn't suitable for ordinal data, for it is agnostic to the order
    of the data. From the above interpretation, it's more similar to fitting
    $J - 1$ individual logisitic regression models. For an ordinal model, we
    might want behavior like the most probable level varies monotonically with
    some covariate. There's no way to model such behavior with the
    \emph{generalized logit model} since each class has separate paramters.
  \end{description}
\item Let
  \begin{equation}
    \pi_{ij} = \mathbb{P}\left(Y_i \leq j \mid \mathbf{x}_i\right),
    \label{eqn:p5_pi_ij}
  \end{equation}
  for $j=0,\ldots,J-2$ and with $\mathbf{x}_i = \begin{pmatrix}
    1 & x_{1i} & x_{2i} & \cdots & x_{ki}
  \end{pmatrix}^\intercal$. Consider the proportional odds model
  \begin{equation}
    \log\frac{\pi_{ij}}{1 - \pi_{ij}} = \alpha_j - \mathbf{x}_i^\intercal\bm\beta,
  \end{equation}
  for $j = 0,1,J - 2$, and where $\bm\beta = \begin{pmatrix}
    \beta_0 & \beta_1 & \cdots & \beta_k
  \end{pmatrix}^\intercal$. Write down, in as simplified a form as possible, the
  log-likelihood $l\left(\bm\alpha,\bm\beta\right)$ where
  $\bm\alpha = \begin{pmatrix}
    \alpha_0 & \alpha_1 & \cdots & \alpha_{J - 2}
  \end{pmatrix}^\intercal$, for the sample $y_i$, $i = 1,\ldots,n$.
  \begin{description}
  \item[Solution:] 
  \end{description}
\item For the data in Table \ref{tab:p1_data}, provide a single plot that shows
  the association between $\mathbf{p}\left(\mathbf{x}_i\right)$ and $x_{1i}$ and
  $x_{2i}$, on a scale you feel is appropriate.
  \begin{description}
  \item[Solution:] 
  \end{description}
\item Fit the proportional odds models:
  \begin{align}
    \log\frac{\pi_{ij}}{1 - \pi_{ij}}
    &=
      \label{eqn:p7_model_1} \\
    &=
      \label{eqn:p7_model_2} \\
    &=
      \label{eqn:p7_model_3} \\
    &= .
      \label{eqn:p7_model_4}
  \end{align}
  Compare
\end{enumerate}
\begin{table}
  \small
  \centering
  \input{p1_data.tex}
  \caption{Data on mental impairment, socioeconomic status (SES) and life
    events, for 40 subjects.}
  \label{tab:p1_data}
\end{table}
\end{document}
