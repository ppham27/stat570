\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[hmargin=1.25in,vmargin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{pdflscape}
\usepackage{subcaption}

\title{Coursework 8: STAT 570}
\author{Philip Pham}
\date{\today}

\begin{document}
\maketitle
\begin{enumerate}
\item Consider n experiments with $Z_{ij}$ , $j = 1,2,\ldots, N_i$, the binary
  outcomes within cluster (experiment) $i$ with $Y_i = \sum^N_{j=1} Z_{ij}$,
  $i = 1,\ldots,n$.
  \begin{enumerate}
  \item By writing
    \begin{equation}
      \operatorname{var}\left(Y_i\right)
      = \sum_{j=1}^{N_i} \operatorname{var}\left(Z_{ij}\right)
      + \sum_{j=1}^{N_i}\sum_{j \neq k} \operatorname{cov}\left(Z_{ij}, Z_{ik}\right),
      \label{eqn:p1_var_yi}
    \end{equation}
    show that
    \begin{equation}
      \operatorname{var}\left(Y_i\right)
      = N_ip_i\left(1 - p_i\right) \times
      \left[1 + \left(N_i - 1\right)\tau_i^2\right],
      \label{eqn:p1_var_yi_result}
    \end{equation}
    where $p_i = \mathbb{E}\left[Z_{ij}\right]$ and $\tau_i^2$ is the
    correlation of outcomes within cluster $i$.

    \begin{description}
    \item[Solution:] Using the variance for a Bernoulli random variable and the
      definition of the correlation coefficient, we have that
      \begin{align}
        \operatorname{var}\left(Z_{ij}\right)
        &= p_i\left(1 - p_i\right) \label{eqn:p1_var_cov}\\
        \operatorname{cov}\left(Z_{ij}, Z_{ik}\right)
        &= \tau_i^2p_i\left(1 - p_i\right)~\text{for}~j \neq k.
          \nonumber
      \end{align}
      
      Since $Z_{ij}$ are identically distributed for different $j$, we can
      rewrite Equation \ref{eqn:p1_var_yi} as
      \begin{equation}
        \operatorname{var}\left(Y_i\right)
        = N_i\operatorname{var}\left(Z_{i1}\right) +
        N_i\left(N_i - 1\right)\operatorname{cov}\left(Z_{i1},Z_{i2}\right).
        \label{eqn:p1_var_yi_simplified} 
      \end{equation}

      Applying Equation \ref{eqn:p1_var_cov} to Equation
      \ref{eqn:p1_var_yi_simplified}, we have the result
      \begin{align*}
        \operatorname{var}\left(Y_i\right)
        &= N_ip_i\left(1-p_i\right)
          + N_i\left(N_i - 1\right) \tau_i^2p_i\left(1-p_i\right) \\
        &= N_ip_i\left(1-p_i\right) \times \left[
          1 + \left(N_i - 1\right) \tau_i^2
          \right]
      \end{align*}
      as desired.
    \end{description}
  \item Consider the model
    \begin{align}
      Y_i \mid q_i &\sim \operatorname{Binomial}\left(N_i,q_i\right) \\
      q_i &\sim \operatorname{Beta}\left(a_i,b_i\right),
            \label{eqn:p1_model}
    \end{align}
    where we can parameterize as $a_i = dp_i$, $b_i = d\left(1-p_i\right)$, so
    that
    \begin{align}
      \mathbb{E}\left[q_i\right]
      &= p_i = \frac{a_i}{d} \\
      \operatorname{var}\left(q_i\right)
      &= \frac{p_1\left(1 - p_i\right)}{d+1}.
    \end{align}
    Obtain the marginal moments and show that the variance is of the form in
    Equation \ref{eqn:p1_var_yi_result}, and identify $\tau_i^2$.

    \begin{description}
    \item[Solution:] We have that
      \begin{align}
        \mathbb{P}\left(Y_i = y\right)
        &= \int_0^1 \mathbb{P}\left(Y_i \mid q_i\right)p\left(q_i\right)
          \,\mathrm{d}q_i \nonumber\\
        &= \int_0^1 \left(
          {N_i \choose y }q_i^y \left(1 - q_i\right)^{N_i - y}
          \right)
          \left(
          \frac{\Gamma\left(a_i + b_i\right)}
          {\Gamma\left(a_i\right)\Gamma\left(b_i\right)}
          q_i^{a_i - 1}\left(1 - q_i\right)^{b_i - 1}
          \right)
          \,\mathrm{d}q_i \nonumber\\
        &= {N_i \choose y}
          \frac{\Gamma\left(a_i + b_i\right)}
          {\Gamma\left(a_i\right)\Gamma\left(b_i\right)}
          \int_0^1 
          q_i^{a_i + y - 1} \left(1 - q_i\right)^{b_i + N_i - y - 1}
          \,\mathrm{d}q_i \nonumber\\
        &= {N_i \choose y}
          \left(
          \frac{\Gamma\left(a_i + b_i\right)}
          {\Gamma\left(a_i\right)\Gamma\left(b_i\right)}
          \right)
          \left(
          \frac{\Gamma\left(y + a_i\right)\Gamma\left(
          N_i - y + b_i
          \right)}
          {\Gamma\left(N_i + a_i + b_i\right)}
          \right),
          \label{eqn:p1_beta_binomial}
      \end{align}
      so $Y_i \sim \operatorname{BetaBinomial}\left(
        N_i, a_i, b_i\right)$.

      Using Equation \ref{eqn:p1_beta_binomial}, the expectation of $Y_i$ is
      \begin{equation}
        \mathbb{E}\left[Y_i\right]
        = \sum_{y=0}^{N_i}y\mathbb{P}\left(
        Y_i = y
          \right)
          = \sum_{y=1}^{N_i}y\mathbb{P}\left(
          Y_i = y
        \right).
        \label{eqn:p1_expectation_definition}
      \end{equation}

      Note that when $N_i = 1$, Equation \ref{eqn:p1_expectation_definition}
      trivially becomes $a_i/\left(a_i + b_i\right)$. In general, we can show
      that $\displaystyle\mathbb{E}\left[Y_i\right] = N_i\frac{a_i}{a_i +
        b_i}$. With the $N_i = 1$ base case established, we now have
      % \tiny{
        \begin{align}
          \mathbb{E}\left[Y_i\right]
        &= \sum_{y=1}^{N_i}y\mathbb{P}\left(Y_i = y\right) \nonumber\\
        &= \sum_{y=1}^{N_i} y {N_i \choose y}
          \left(
          \frac{\Gamma\left(a_i + b_i\right)}
          {\Gamma\left(a_i\right)\Gamma\left(b_i\right)}
          \right)
          \left(
          \frac{\Gamma\left(y + a_i\right)\Gamma\left(
          N_i - y + b_i
          \right)}
          {\Gamma\left(N_i + a_i + b_i\right)}
          \right) \nonumber\\
          &= \frac{N_i}{N_i - 1 + a_i + b_i} \sum_{y=1}^{N_i}
            \left(y - 1 + a_i\right)
            \operatorname{BetaBinomial}_{N_i - 1,a_i,b_i}
            \left(y - 1\right) \nonumber\\
        &= \frac{N_i}{N_i - 1 + a_i + b_i}
          \left(
          \frac{\left(N_i - 1\right)a_i}{a_i + b_i} + a_i
          \right) = N_i\frac{a_i}{a_i + b_i} 
          \label{eqn:p1_expectation_derivation}
        \end{align}
      % }
        \normalsize as expected. Substituting $a_i = dp_i$ and
        $b_i = d\left(1-p_i\right)$, we have that
      \begin{equation}
        \mathbb{E}\left[
          Y_i \right]
        = N_i\frac{dp_i}{dp_i + d\left(1 - p_i\right)}  = N_ip_i.
        \label{eqn:p1_expectation_substitution}
      \end{equation}
      
      For the variance, we can use the law of total variance to obtain
      \begin{align}
        \operatorname{var}\left(
        Y_i
        \right)
        &= \mathbb{E}\left[
          \operatorname{var}\left(Y_i \mid q_i\right)
          \right] +
          \operatorname{var}\left(\mathbb{E}\left[Y_i \mid q_i\right]\right)
          \nonumber\\
        &= \mathbb{E}\left[N_i q_i\left(1 - q_i\right)\right]
          + \operatorname{var}\left(N_iq_i\right) \nonumber\\
        &= N_i\left(
          \frac{a_i}{a_i + b_i}
          - \left(
          \frac{a_ib_i}{\left(a_i + b_i\right)^2\left(a_i + b_i + 1\right)}
          +  \left(\frac{a_i}{a_i + b_i}\right)^2
          \right)
          \right) \nonumber\\
        &~~~+ N_i^2\frac{a_ib_i}{\left(a_i + b_i\right)^2\left(a_i + b_i + 1\right)}
          \nonumber\\
        &= N_i\frac{a_ib_i\left(a_i+b_i+ N_i\right)}{
          \left(a_i + b_i\right)^2\left(a_i + b_i + 1\right)}.
          \label{eqn:p1_total_variance}
      \end{align}

      From Equations \ref{eqn:p1_expectation_derivation} and
      \ref{eqn:p1_total_variance}, we obtain the second moment
      \begin{align*}
        \mathbb{E}\left[Y_i^2\right]
        &= \operatorname{var}\left(Y_i\right) +
          \left(\mathbb{E}\left[Y_i\right]\right)^2 \\
        &= N_i\frac{a_ib_i\left(a_i+b_i+ N_i\right)}{
          \left(a_i + b_i\right)^2\left(a_i + b_i + 1\right)}
          + \left(N_i\frac{a_i}{a_i + b_i}\right)^2 \\
        &= N_i\frac{a_i\left(N_i\left(a_i + 1\right) + b_i\right)}{
          \left(a_i + b_i\right)\left(a_i + b_i + 1\right)}.
      \end{align*}

      Substituting $a_i = dp_i$ and $b_i = d\left(1-p_i\right)$ into Equation
      \ref{eqn:p1_total_variance}, we have
      \begin{align}
        \operatorname{var}\left(Y_i\right)
        &= N_ip_i\left(1 - p_i\right)\frac{a_i + b_i + N_i}{a_i + b_i + 1}
          \nonumber\\
        &= N_ip_i\left(1 - p_i\right)\frac{a_i + b_i + 1 + \left(N_i - 1\right)}
          {a_i + b_i + 1} \nonumber\\
        &= N_ip_i\left(1 - p_i\right) \times \left[
          1 + \left(N_i - 1\right)\frac{1}{d + 1}\right].
          \label{eqn:p1_variance_substitution}
      \end{align}

      Thus, we have that $\tau^2 = 1/\left(d + 1\right)$, so small values of $d$
      mean that the $Z_{ij}$ are highly correlated. This is consistent with the
      behavior of the beta distribution since for small $d$, $q_i$ is likely to
      be close to $0$ and $1$.
    \end{description}
\end{enumerate}
\item In this question a simulation study to investigate the impact on inference
  of omitting covariates in logistic regression will be performed, in the
  situation in which the covariates are independent of the exposure of
  interest. Let $x$ be the covariate of interest and $z$ another
  covariate. Suppose the true (adjusted) model is independently distributed
  $Y_i \mid x_i,z_i \sim \operatorname{Bernoulli}\left(p_i\right)$, where
  \begin{equation}
    \log\frac{p_i}{1 - p_i} = \beta_0 + \beta_1x_i + \beta_2z_i.
    \label{eqn:p2_pi}.
  \end{equation}
  A comparison with the unadjusted model
  $Y_i \mid x_i \sim \operatorname{Bernoulli}\left(p_i^\star\right)$
  independently distributed, where
  \begin{equation}
    \log\frac{p_i^\star}{1 - p_i^\star} = \beta_0^\star + \beta_1^\star x_i,
    \label{eqn:p2_pi_star}
  \end{equation}

  for $i = 1,\ldots,n=1000$ will be made. Suppose $x$ is binary with
  $\mathbb{P}\left(X = 1\right) = 0.5$ and $Z \sim \mathcal{N}\left(0, 1\right)$
  independent and identically distributed with $x$ and $z$
  independent. Combinations of the parameters
  $\beta_1 \in \left\{ 0.5, 1 \right\}$ and
  $\beta_2 \in \left\{0.5,1.0,2.0,3.0\right\}$, with $\beta_0 = -2$ in all cases
  will be considered.

  For each combination of parameters compare the results from the two models,
  Equation \ref{eqn:p2_pi} and Equation \ref{eqn:p2_pi_star}, with respect to:
  \begin{enumerate}
  \item $\mathbb{E}\left[\hat{\beta}_1\right]$ and
    $\mathbb{E}\left[\hat{\beta}_1^\star\right]$, as compared to $\beta_1$.
  \item The standard errors of $\hat{\beta}_1$ and $\hat{\beta}_1^\star$.
  \item The coverage of 95\% confidence intervals for $\beta_1$ and
    $\beta_1^\star$.
  \item The probability of rejecting $H_0 : \beta_1 = 0$ (the power) under both
    models using a Wald test.
  \end{enumerate}

  Based on the results, summarize the effect of omitting a covariate that is
  independent of the exposure of interest, in particular in comparison with the
  linear model.

  \begin{description}
  \item[Solution:] 
  \end{description}
\end{enumerate}
\end{document}
